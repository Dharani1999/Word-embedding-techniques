{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Code_model_v2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1wgpB7rU7rrk4BnoapdXtGyFmepqlRw6K",
      "authorship_tag": "ABX9TyOIJfBld4bsBF+8d7rRadlJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dharani1999/Word-embedding-techniques/blob/master/Code_model_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTPPDaIkL-DE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "outputId": "00d44bbb-54a7-438d-9f0d-4864770290e3"
      },
      "source": [
        "import pickle\n",
        "import gensim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from gensim.parsing.preprocessing import stem_text\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "from gensim import corpora, models, similarities\n",
        "from gensim.models import Word2Vec, TfidfModel, LsiModel\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from gensim.models.fasttext import FastText\n",
        "\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorboard as tb\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Activation,Dense,Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.python.framework import ops\n",
        "\n",
        "import random\n",
        "import pathlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "!pip install glove_python\n",
        "from glove import Corpus, Glove # creating a corpus object"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Requirement already satisfied: glove_python in /usr/local/lib/python3.6/dist-packages (0.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from glove_python) (1.18.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from glove_python) (1.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nrx8P2P32kT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_object(obj, filename):\n",
        "    with open(filename, 'wb') as output:  # Overwrites any existing file.\n",
        "        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "def load_object(filename):\n",
        "    with open(filename, 'rb') as input:   # loads an object from a given location\n",
        "        pickle_object = pickle.load(input)\n",
        "    return  pickle_object"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtHGs7qlVUsn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "3c8d5aa6-3db0-4d34-c0b8-a865351b3555"
      },
      "source": [
        "def pre_process(data):\n",
        "  _sent=[]\n",
        "  for i in data:\n",
        "    sent = word_tokenize(i)\n",
        "    sent = [w.lower() for w in sent]\n",
        "    sent = [word for word in sent if word not in stop_words]\n",
        "    sent = [word for word in sent if word.isalpha()]\n",
        "    sent = [c for c in sent if not c.isdigit()]\n",
        "    sent = [stem_text(w) for w in sent]\n",
        "    #sent = [wordnet_lemmatizer.lemmatize(w) for w in sent]\n",
        "    _sent.append(sent)\n",
        "  return _sent\n",
        "\n",
        "# Give the locations to save the data and word_to_idx_dict\n",
        "# skip_top skips the given number of most frequent words\n",
        "# num_words skips words less frequent than given value\n",
        "\n",
        "from keras.datasets import imdb\n",
        "data = imdb.load_data(path='/content/drive/My Drive/Movielensdata/ml25m/imdb/data',\n",
        "    num_words=None,\n",
        "    skip_top=0,\n",
        "    maxlen=None,\n",
        "    seed=113,\n",
        "    start_char=1,\n",
        "    oov_char=2,\n",
        "    index_from=3)\n",
        "\n",
        "save_object(data, '/content/drive/My Drive/Movielensdata/ml25m/imdb/data')\n",
        "word_to_idx_dict = imdb.get_word_index(path=\"/content/drive/My Drive/Movielensdata/ml25m/imdb/imdb_word_index.json\")"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A local file was found, but it seems to be incomplete or outdated because the auto file hash does not match the original value of 599dadb1135973df5b59232a0e9a887c so we will re-download the data.\n",
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EslXGch8IZgd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_module(reviews, name, location, output_location, dict_location, tfidf_features=10000, max_seq_length=100, train_data_loc = '/content/drive/My Drive/Movielensdata/ml25m/imdb/processed_data/train_sent', test_data_loc = '/content/drive/My Drive/Movielensdata/ml25m/imdb/processed_data/test_sent', val_data_loc = '/content/drive/My Drive/Movielensdata/ml25m/imdb/processed_data/val_sent', Y_train_loc ='/content/drive/My Drive/Movielensdata/ml25m/imdb/Y/Y_train', Y_test_loc = '/content/drive/My Drive/Movielensdata/ml25m/imdb/Y/Y_test', Y_val_loc = '/content/drive/My Drive/Movielensdata/ml25m/imdb/Y/Y_val'):\n",
        "  if reviews==False:\n",
        "    # Loads ratings.csv file from the given location\n",
        "    dataset1 = pd.read_csv(location)\n",
        "    dataset = dataset1.iloc[0:1000,:]\n",
        "  else:\n",
        "    # Loads the imdb data from the given location\n",
        "    dataset = load_object(location)\n",
        "  if name == 'word2vec':\n",
        "    data_corpus, users_total = data_word2vec(reviews, dataset, max_seq_length, train_loc = train_data_loc, test_loc = test_data_loc, val_loc = val_data_loc, y_train = Y_train_loc, y_test = Y_test_loc, y_val = Y_val_loc)\n",
        "  elif name == 'doc2vec':\n",
        "    data_corpus, users_total = data_doc2vec(reviews, dataset, max_seq_length, train_loc = train_data_loc, test_loc = test_data_loc, val_loc = val_data_loc, y_train = Y_train_loc, y_test = Y_test_loc, y_val = Y_val_loc)\n",
        "  elif name == 'lsi':\n",
        "    data_corpus, users_total = data_lsi(reviews, dataset, max_seq_length, dict_loc=dict_location, train_loc = train_data_loc, test_loc = test_data_loc, val_loc = val_data_loc, y_train = Y_train_loc, y_test = Y_test_loc, y_val = Y_val_loc)\n",
        "  elif name == 'tfidf':\n",
        "    data_corpus, users_total = data_tfidf(reviews, dataset, max_seq_length, tfidf_features, dict_loc=dict_location, train_loc = train_data_loc, test_loc = test_data_loc, val_loc = val_data_loc, y_train = Y_train_loc, y_test = Y_test_loc, y_val = Y_val_loc)\n",
        "  elif name == 'glove':\n",
        "    data_corpus, users_total = data_word2vec(reviews, dataset, max_seq_length, train_loc = train_data_loc, test_loc = test_data_loc, val_loc = val_data_loc, y_train = Y_train_loc, y_test = Y_test_loc, y_val = Y_val_loc)\n",
        "  elif name == 'hashing':\n",
        "    data_corpus, users_total = data_hashing(reviews, dataset, max_seq_length, train_loc = train_data_loc, test_loc = test_data_loc, val_loc = val_data_loc, y_train = Y_train_loc, y_test = Y_test_loc, y_val = Y_val_loc)\n",
        "  elif name == 'cooccur':\n",
        "    data_corpus, users_total = data_hashing(reviews, dataset, max_seq_length, train_loc = train_data_loc, test_loc = test_data_loc, val_loc = val_data_loc, y_train = Y_train_loc, y_test = Y_test_loc, y_val = Y_val_loc)\n",
        "  elif name == 'fasttext':\n",
        "    data_corpus, users_total = data_word2vec(reviews, dataset, max_seq_length, train_loc = train_data_loc, test_loc = test_data_loc, val_loc = val_data_loc, y_train = Y_train_loc, y_test = Y_test_loc, y_val = Y_val_loc)\n",
        "  \n",
        "  # Saves the processed train data in the given output location\n",
        "  save_object(obj=data_corpus, filename=output_location)\n",
        "\n",
        "def data_word2vec(review,dataset,max_seq_length, train_loc, test_loc, val_loc, y_train, y_test, y_val):\n",
        "  if review==False:\n",
        "    dataset.sort_values(by=['userId','timestamp'],inplace=True)\n",
        "    user_total = len(dataset['userId'].unique())\n",
        "    \n",
        "    #Selecting the most recent movies rated by each user and padding if necessary\n",
        "    movie_list = []\n",
        "    for i in range(user_total):\n",
        "      list1 = []\n",
        "      list1 = dataset.loc[dataset['userId'] ==(i+1),['movieId']]['movieId'].tolist()\n",
        "      if len(list1)>max_seq_length:\n",
        "        list1 = list1[(len(list1)-max_seq_length):]\n",
        "      elif len(list1)<max_seq_length:\n",
        "        list1 = list1+[0 for j in range((max_seq_length-len(list1)))]\n",
        "      movie_list.append(list1)\n",
        "\n",
        "    #Selecting the most recent ratings rated by each user and padding if necessary\n",
        "    rating_list =[]\n",
        "    for i in range(user_total):\n",
        "      list2 = []\n",
        "      list2 = dataset.loc[dataset['userId'] ==(i+1),['rating']]['rating'].tolist()\n",
        "      if len(list2)>max_seq_length:\n",
        "        list2 = list2[(len(list2)-max_seq_length):]\n",
        "      elif len(list2)<max_seq_length:\n",
        "        list2 = list2+[0 for j in range((max_seq_length-len(list2)))]\n",
        "      rating_list.append(list2)\n",
        "\n",
        "    #Creating user_id level transpose matrices\n",
        "    movies_transpose = pd.DataFrame(data=movie_list,index=[i+1 for i in range(user_total)])\n",
        "    movies_transpose.index.names = ['userId']\n",
        "\n",
        "    ratings_transpose = pd.DataFrame(data=rating_list,index=[i+1 for i in range(user_total)])\n",
        "    ratings_transpose.index.names = ['userId']\n",
        "\n",
        "    # Select features from original dataset to form a new dataframe \n",
        "    df1 = movies_transpose.iloc[:] # For each row, combine all the columns into one column\n",
        "    df2 = df1.apply(lambda x: ','.join(x.astype(str)), axis=1) # Store them in a pandas dataframe\n",
        "    df_clean = pd.DataFrame({'clean': df2}) # Create the list of list format of the custom corpus for gensim modeling \n",
        "    sent = [row.split(',') for row in df_clean['clean']]\n",
        "\n",
        "    # returns list of lists format data and the number of users\n",
        "    return sent, user_total\n",
        "  else:\n",
        "    X_train = dataset[0][0]\n",
        "    Y_train = dataset[0][1]\n",
        "    # test-val split of data\n",
        "    X_test, X_val, Y_test, Y_val = train_test_split(dataset[1][0], dataset[1][1], test_size=0.4, shuffle=False)\n",
        "    \n",
        "    #train_seq = X_train.tolist()\n",
        "    #test_seq = X_test.tolist()\n",
        "    #val_seq = X_val.tolist()\n",
        "    #train_sent = pad_sequences(train_seq, padding=\"post\", truncating=\"post\", maxlen=200).tolist()\n",
        "    #test_sent = pad_sequences(test_seq, padding=\"post\", truncating=\"post\", maxlen=200).tolist()\n",
        "    #val_sent = pad_sequences(val_seq, padding=\"post\", truncating=\"post\", maxlen=200).tolist()\n",
        "    \n",
        "    # Decoding reviews from given integer labels\n",
        "    reverse_word_index = dict([(value, key) for (key, value) in word_to_idx_dict.items()])\n",
        "    \n",
        "    train_reviews=[]\n",
        "    for j in X_train:\n",
        "      train_reviews.append(' '.join([reverse_word_index.get(i - 3, '#') for i in j]))\n",
        "\n",
        "    test_reviews=[]\n",
        "    for j in X_test:\n",
        "      test_reviews.append(' '.join([reverse_word_index.get(i - 3, '#') for i in j]))\n",
        "\n",
        "    val_reviews=[]\n",
        "    for j in X_val:\n",
        "      val_reviews.append(' '.join([reverse_word_index.get(i - 3, '#') for i in j]))\n",
        "    \n",
        "    # pre-processing train, test, val reviews and saving in given locations\n",
        "    train_sent=pre_process(train_reviews)\n",
        "    save_object(obj = train_sent, filename = train_loc)\n",
        "\n",
        "    test_sent=pre_process(test_reviews)\n",
        "    save_object(obj = test_sent, filename = test_loc)\n",
        "\n",
        "    val_sent=pre_process(val_reviews)\n",
        "    save_object(obj = val_sent, filename = val_loc)\n",
        "\n",
        "    save_object(obj = Y_train, filename = y_train)\n",
        "    save_object(obj = Y_test, filename = y_test)\n",
        "    save_object(obj = Y_val, filename = y_val)\n",
        "\n",
        "    # returns processed train reviews and the number of train reviews\n",
        "    return train_sent, len(train_sent)\n",
        "\n",
        "def data_doc2vec(review, dataset, max_seq_length, train_loc, test_loc, val_loc, y_train, y_test, y_val):\n",
        "  # Takes the processed train reviews and adds a tag for each review\n",
        "  Sent, user_total = data_word2vec(review, dataset, max_seq_length, train_loc, test_loc, val_loc, y_train, y_test, y_val)\n",
        "  tagged_data = []\n",
        "  tags = []\n",
        "  for i in range(user_total):\n",
        "    tagged_data = tagged_data + [TaggedDocument(words=Sent[i], tags=[str(i)])]\n",
        "  # returns tagged_data and number of users or reviews\n",
        "  return tagged_data, user_total\n",
        "\n",
        "def data_lsi(review, dataset, max_seq_length, dict_loc, train_loc, test_loc, val_loc, y_train, y_test, y_val):\n",
        "  # Takes the processed train reviews and adds a tag for each review\n",
        "  Sent, user_total = data_word2vec(review, dataset, max_seq_length, train_loc, test_loc, val_loc, y_train, y_test, y_val)\n",
        "\n",
        "  # Creating a dictionary of all the unique words or tokens\n",
        "  dictionary = corpora.Dictionary(Sent)\n",
        "\n",
        "  # Getting a corpus with Bag of Words format and saving the dict in the given location\n",
        "  corpus = [dictionary.doc2bow(text) for text in Sent]\n",
        "  dictionary.save(dict_loc)\n",
        "\n",
        "  return corpus, user_total\n",
        "\n",
        "def data_tfidf(review, dataset, max_seq_length, max_features, dict_loc, train_loc, test_loc, val_loc, y_train, y_test, y_val):\n",
        "  # Takes the processed train reviews and adds a tag for each review\n",
        "  Sent, user_total = data_word2vec(review, dataset, max_seq_length, train_loc, test_loc, val_loc, y_train, y_test, y_val)\n",
        "\n",
        "  # Creating a dictionary using all the unique tokens or words\n",
        "  dictionary = corpora.Dictionary(Sent)\n",
        "\n",
        "  # Filter out tokens that appear in \n",
        "  # less than no_below documents,\n",
        "  # more than no_above documents and\n",
        "  # keeping only the first keep_n most frequent tokens\n",
        "\n",
        "  dictionary.filter_extremes(no_below=10, no_above=0.5, keep_n=max_features)\n",
        "\n",
        "  # Getting a corpus with Bag of Words format and saving the dict in the given location\n",
        "  corpus = [dictionary.doc2bow(text) for text in Sent]\n",
        "  dictionary.save(dict_loc)\n",
        "\n",
        "  return corpus, user_total\n",
        "\n",
        "def data_hashing(review, dataset, max_seq_length, train_loc, test_loc, val_loc, y_train, y_test, y_val):\n",
        "  # Takes the processed train reviews and adds a tag for each review\n",
        "  Sent, user_total = data_word2vec(review, dataset, max_seq_length, train_loc, test_loc, val_loc, y_train, y_test, y_val)\n",
        "\n",
        "  # Converting list of lists format to list of strings format to suit the input requirement\n",
        "  corpus = [str(str(doc)[1:-1]) for doc in Sent]\n",
        "  return corpus, user_total"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kJytoDNK3E0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def embedding_model(name,Data_location,model_save_location,matrix_location, cooccur_features, vector_dims=10,Sg=1,size_window=3,topics=10,mini_count=1,num_workers=3,max_num_epochs = 100,alpha = 0.025,min_alpha=0.00025,dm=1):\n",
        "  Data = load_object(Data_location)\n",
        "  if name == 'word2vec':\n",
        "    word2vec(input_data=Data, save_loc=model_save_location, vec_dims=vector_dims, SG=Sg, size_of_window=size_window, minimum_count=mini_count, no_workers=num_workers)\n",
        "  elif name == 'doc2vec':\n",
        "    doc2vec(input_data=Data, save_loc=model_save_location, vec_dims=vector_dims, alpha_=alpha, size_of_window=size_window, no_workers=num_workers, max_epochs=max_num_epochs,min_alpha_=min_alpha, minimum_count=mini_count, dms=dm)\n",
        "  elif name == 'lsi':\n",
        "    lsi(input_data=Data, save_loc=model_save_location, total_topics=topics)\n",
        "  elif name == 'tfidf':\n",
        "    tfidf(input_data=Data,save_loc=model_save_location)\n",
        "  elif name == 'glove':\n",
        "    glove_model(input_data=Data, vec_dims=vector_dims, size_of_window=size_window, save_loc=model_save_location, num_epochs=max_num_epochs, alpha_=0.05, num_threads=4)\n",
        "  elif name == 'hashing':\n",
        "    hashing(input_data=Data, vec_dims=vector_dims, save_loc=model_save_location)\n",
        "  elif name == 'cooccur':\n",
        "    co_occur(input_data=Data, maximum_features=cooccur_features, save_loc=model_save_location, matrix_loc = matrix_location)\n",
        "  elif name == 'fasttext':\n",
        "    fast_text(input_data=Data, save_loc=model_save_location, vec_dims=vector_dims, SG=Sg, size_of_window=size_window, minimum_count=mini_count, no_workers=num_workers, alpha_=0.025)\n",
        "# doesn't return anything, model is trained and saved in given location\n",
        "\n",
        "def word2vec(input_data,save_loc,vec_dims,SG,size_of_window,minimum_count,no_workers):\n",
        "  # Trains a word2vec model on the training data and saves in the given location\n",
        "  model = Word2Vec(input_data,min_count=minimum_count,size= vec_dims,workers=no_workers, window =size_of_window, sg = SG)\n",
        "  model.save(save_loc)\n",
        "\n",
        "def doc2vec(input_data,save_loc,vec_dims,alpha_,size_of_window,min_alpha_,minimum_count,dms,no_workers,max_epochs):\n",
        "  # Trains a doc2vec model on the training data and saves in given location\n",
        "  model = Doc2Vec(size=vec_dims,\n",
        "                alpha=alpha_, \n",
        "                min_alpha=min_alpha_,\n",
        "                window = size_of_window,\n",
        "                min_count=minimum_count,\n",
        "                dm =dms)\n",
        "  \n",
        "  model.build_vocab(input_data)\n",
        "  for epoch in range(max_epochs):\n",
        "    model.train(input_data, total_examples=model.corpus_count, epochs=model.iter)\n",
        "    # decrease the learning rate\n",
        "    model.alpha -= 0.0002\n",
        "    # fix the learning rate, no decay\n",
        "    model.min_alpha = model.alpha\n",
        "  model.save(save_loc)\n",
        "\n",
        "def lsi(input_data,save_loc,total_topics):\n",
        "  # Trains an LSI model on the training corpus and saves in the given location\n",
        "  model = models.LsiModel(corpus=input_data, num_topics=total_topics)\n",
        "  model.save(save_loc)\n",
        "\n",
        "def tfidf(input_data,save_loc):\n",
        "  # Trains a Tf-idf model on the training corpus and saves in the given location\n",
        "  model = models.TfidfModel(corpus=input_data, normalize=True)\n",
        "  model.save(save_loc)\n",
        "\n",
        "def glove_model(input_data,vec_dims,size_of_window,save_loc,alpha_=0.05,num_epochs=30, num_threads=4):\n",
        "  #training the corpus to generate the co occurence matrix which is used in GloVe\n",
        "  corpus = Corpus() \n",
        "  corpus.fit(input_data, window=size_of_window)\n",
        "\n",
        "  #creating a Glove object which will use the matrix created in the above lines to create embeddings\n",
        "  # And saving the model in given location\n",
        "  glove = Glove(no_components=vec_dims, learning_rate=alpha_) \n",
        "  glove.fit(corpus.matrix, epochs=num_epochs, no_threads=4, verbose=True)\n",
        "  glove.add_dictionary(corpus.dictionary)\n",
        "  glove.save(save_loc)\n",
        "\n",
        "def fast_text(input_data,save_loc,vec_dims,SG,size_of_window,minimum_count,no_workers,alpha_=0.025):\n",
        "  # Trains a fasttext model on the training data and saves in the given location\n",
        "  model = FastText(min_count=minimum_count, alpha=alpha_, size= vec_dims, workers=no_workers, window =size_of_window)\n",
        "  model.build_vocab(input_data)\n",
        "  model.train(input_data, epochs=model.epochs, total_examples=model.corpus_count, total_words=model.corpus_total_words)\n",
        "  model.save(save_loc)\n",
        "\n",
        "def hashing(input_data,vec_dims,save_loc):\n",
        "  # Calls a hashing vectorizer on the training data and saves in the given location\n",
        "  model = HashingVectorizer(n_features=vec_dims)\n",
        "  model.transform(input_data)\n",
        "  save_object(obj=model, filename=save_loc)\n",
        "\n",
        "def co_occur(input_data, save_loc, matrix_loc, maximum_features):\n",
        "  # Calling a Count Vectorizer to create a document-term matrix\n",
        "  model = CountVectorizer(max_df=0.5, ngram_range=(1,1),max_features=maximum_features, token_pattern= r\"(?u)\\b\\w+\\b\")\n",
        "  X = model.fit_transform(input_data)\n",
        "\n",
        "  # Getting the term- term co occurrence matrix from doc-term matrix\n",
        "  Xc = (X.T * X)\n",
        "  Xc.setdiag(0)\n",
        "  # Creating a pandas dataframe with index as the terms, columns as the terms \n",
        "  # and each row representing a term's co occurrence frequencies with other terms\n",
        "  df = pd.DataFrame(data = Xc.toarray(), columns = names, index = names)\n",
        "  # Saving the model and Dataframe in given locations\n",
        "  save_object(obj=model, filename=save_loc)\n",
        "  save_object(obj=df, filename=matrix_loc)\n"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXlM6wLvQVIW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scoring_module(name,model_loc,data_loc,dict_loc,matrix_location):\n",
        "  data = load_object(data_loc)\n",
        "  if name == 'word2vec':\n",
        "    scores_list = score_word2vec(model_location=model_loc,input_data=data)\n",
        "  elif name == 'doc2vec':\n",
        "    scores_list = score_doc2vec(model_location=model_loc,input_data=data)\n",
        "  elif name == 'lsi':\n",
        "    scores_list = score_lsi(model_location=model_loc,input_data=data, dict_location=dict_loc)\n",
        "  elif name == 'tfidf':\n",
        "    scores_list = score_tfidf(model_location=model_loc,input_data=data, dict_location=dict_loc)\n",
        "  elif name == 'glove':\n",
        "    scores_list = score_glove(model_location=model_loc, input_data=data)\n",
        "  elif name == 'hashing':\n",
        "    scores_list = score_hashing(model_location=model_loc, input_data=data)\n",
        "  elif name == 'cooccur':\n",
        "    scores_list = score_cooccur(matrix_loc=matrix_location, input_data=data)\n",
        "  elif name == 'fasttext':\n",
        "    scores_list = score_fasttext(model_location=model_loc, input_data=data)\n",
        "  return scores_list\n",
        "\n",
        "def score_word2vec(model_location,input_data):\n",
        "  # Loads the trained word2vec model from given location\n",
        "  model = Word2Vec.load(model_location)\n",
        "  # Finds embedding for each word and makes a list of all the embeddings\n",
        "  scored_data = [model.wv[text] for text in input_data]\n",
        "  return scored_data\n",
        "\n",
        "def score_doc2vec(model_location,input_data):\n",
        "  # Loads the trained doc2vec model from given location\n",
        "  model = Doc2Vec.load(model_location)\n",
        "  # Finds word embedding for each sentence and makes a list of all\n",
        "  scored_data = [model.infer_vector(text) for text in input_data]\n",
        "  return scored_data\n",
        "\n",
        "def score_glove(model_location,input_data):\n",
        "  # Loads the trained Glove model from given location\n",
        "  model = Glove.load(model_location)\n",
        "  scored_data = []\n",
        "  # Finds embedding for each word and makes a list of all embeddings\n",
        "  for doc in input_data:\n",
        "    list1 = []\n",
        "    for text in doc:\n",
        "      list1.append(model.word_vectors[model.dictionary[text]])\n",
        "    scored_data.append(list1)\n",
        "  return scored_data\n",
        "\n",
        "def score_fasttext(model_location, input_data):\n",
        "  # Loads the trained fasttext model from given location\n",
        "  model = FastText.load(model_location)\n",
        "  # Finds embedding for each word and makes a list of all the embeddings\n",
        "  scored_data = [model.wv[text] for text in input_data]\n",
        "  return scored_data\n",
        "\n",
        "def score_lsi(model_location, input_data, dict_location):\n",
        "  # Loads trained LSI model from given location\n",
        "  model = LsiModel.load(model_location)\n",
        "  # Loads the saved dictionary and makes bag of words representation of the data to be scored\n",
        "  dictionary = corpora.Dictionary.load(dict_location)\n",
        "  scored_data = [model[dictionary.doc2bow(text)] for text in input_data]\n",
        "  scored_dat = np.zeros((len(scored_data),len(scored_data[0])))\n",
        "  # Converts sparse Bag of words embeddings to dense vectors\n",
        "  for i in range(len(scored_data)):\n",
        "    for k in range(len(scored_data[i])):\n",
        "      scored_dat[i][k] = scored_data[i][k][1]\n",
        "  return scored_dat\n",
        "\n",
        "def score_tfidf(model_location, input_data, dict_location):\n",
        "  # Loads trained Tf-idf model from given location\n",
        "  model = TfidfModel.load(model_location)\n",
        "  # Loads the saved dictionary and makes bag of words representation of the data to be scored\n",
        "  dictionary = corpora.Dictionary.load(dict_location)\n",
        "  scored_data = [model[dictionary.doc2bow(text)] for text in input_data]\n",
        "  scored_dat = np.zeros((len(scored_data),len(dictionary)), dtype=np.float64)\n",
        "  # Converts sparse Bag of words embeddings to dense vectors\n",
        "  for i in range(len(scored_data)):\n",
        "    for k in range(len(scored_data[i])):\n",
        "      scored_dat[i][scored_data[i][k][0]] = scored_data[i][k][1]\n",
        "  return scored_dat\n",
        "\n",
        "def score_hashing(model_location, input_data):\n",
        "  # Gets a corpus with list of strings format from the list of lists data\n",
        "  corpus = [str(str(doc)[1:-1]) for doc in input_data]\n",
        "  # Loads the saved model from given location\n",
        "  model = load_object(model_location)\n",
        "  # the corpus is transformed into sparse vectors\n",
        "  X = model.transform(corpus)\n",
        "  Y = X.toarray()\n",
        "  return Y\n",
        "\n",
        "def score_cooccur(matrix_loc, input_data):\n",
        "  # Loads the saved dataframe of term-term co-occurrences\n",
        "  cooccur_matrix = load_object(matrix_loc)\n",
        "  # makes a list of word embeddings for each of the word in the data\n",
        "  scored_data = [cooccur_matrix.loc[text].tolist() for doc in input_data for text in doc]\n",
        "  return scored_data"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgR527dPZlEc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sent_embedding(name,vec_dims,score_list):\n",
        "  if ((name=='word2vec') or (name=='glove') or (name=='fasttext')):\n",
        "    # Takes average of word-level embeddings in a sentence to get sentence embedding\n",
        "    sent_list = []\n",
        "    for i in range(len(score_list)):\n",
        "      sum = np.zeros((vec_dims))\n",
        "      for j in range(len(score_list[i])):\n",
        "        sum = sum + score_list[i][j]\n",
        "      sum = sum/(len(score_list[i]))\n",
        "      sent_list.append(sum)\n",
        "\n",
        "    # convert to tensor\n",
        "    sent_tensor = tf.convert_to_tensor(sent_list, dtype=tf.float32)\n",
        "  elif ((name=='doc2vec') or (name=='tfidf') or (name=='lsi') or (name=='hashing')):\n",
        "    # For these models, we have sentence level embeddings. Just convert to tensor\n",
        "    sent_tensor = tf.convert_to_tensor(score_list, dtype=tf.float32)\n",
        "  return sent_tensor\n",
        "\n",
        "def Model_build(X,Y,X_predict,Y_predict,accuracy_save_location, sent_save_location,batches_size=100):\n",
        "  ops.reset_default_graph()\n",
        "\n",
        "  # The model is trained with dense layers \n",
        "  Model = Sequential()\n",
        "  Model.add(Dense(20, activation='relu'))\n",
        "  Model.add(Dropout(0.3))\n",
        "  Model.add(Dense(5, activation='relu'))\n",
        "  Model.add(Dropout(0.3))\n",
        "  Model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "  # These hyper-parameters can be changed for a better performance\n",
        "  opt = optimizers.Adam(lr=0.005, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "  \n",
        "  Model.compile(loss='binary_crossentropy',optimizer=opt,metrics=['accuracy'])\n",
        "\n",
        "  # Number of steps in each iteration\n",
        "  steps_test = int(len(X)/batches_size)\n",
        "  steps_val = int(len(X_predict)/batches_size)\n",
        "\n",
        "  # Training the model on scored-test-review-embedding-tensors.\n",
        "  Model.fit(X, Y, epochs=10, batch_size=batches_size)\n",
        "  Model.save('/content/drive/My Drive/Movielensdata/ml25m/imdb/model')\n",
        "\n",
        "  # Finding model predictions on validation data and evaluating with Y_val for accuracy\n",
        "  sentiment_score = Model.predict(X_predict, batch_size=batches_size, steps=steps_val, workers=3)\n",
        "  metrics = Model.evaluate(x=X_predict, y=Y_predict, batch_size=batches_size, verbose=1, steps=steps_val, max_queue_size=10, return_dict=False)\n",
        "\n",
        "  # metrics are loss and accuracy\n",
        "  accuracy = metrics[1]\n",
        "  loss = metrics[0]\n",
        "\n",
        "  # Saving sentiment scores and accuracy in given locations\n",
        "  save_object(sentiment_score,sent_save_location)\n",
        "  save_object(accuracy,accuracy_save_location)\n",
        "  return sentiment_score, accuracy"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTON8_HeWKAV",
        "colab_type": "text"
      },
      "source": [
        "1) Name can be 'word2vec' or 'doc2vec' or 'lsi' or 'tfidf' or 'glove' or 'cooccur' or 'hashing' or 'fasttext'.\n",
        "\n",
        "2) location is the path to input file\n",
        "\n",
        "3) output_location is the path to save the processed data\n",
        "\n",
        "4) dict_location is the path to dictionary of LSI or TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXdIQWmIGFOG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#data_module(reviews=False,name='glove',max_seq_length=100,location='/content/drive/My Drive/Movielensdata/ml25m/ratings.csv',output_location='/content/drive/My Drive/Movielensdata/ml25m/glove/data', dict_location='/content/drive/My Drive/Movielensdata/ml25m/tfidf/dict')\n",
        "\n",
        "#embedding_model(name='glove',Data_location='/content/drive/My Drive/Movielensdata/ml25m/glove/data',model_save_location='/content/drive/My Drive/Movielensdata/ml25m/glove/glv', matrix_location='/content/drive/My Drive/Movielensdata/ml25m/cooccur/matrix', vector_dims=10,Sg=1,size_window=3,topics=10,mini_count=1,num_workers=3,max_num_epochs = 10,alpha = 0.025,min_alpha=0.00025,dm=1,maxi_features=None)\n",
        "\n",
        "#scoring_input = [['899', '2161', '3949', '5878', '1175', '1237', '8154', '2843', '7365', '4422', '6016'],['1080', '3114', '3671', '2791', '1288', '1', '541', '2692', '7323', '8014', '6370', '4703', '5147']]\n",
        "#save_object(scoring_input,'/content/drive/My Drive/Movielensdata/ml25m/scoring_input')\n",
        "\n",
        "#scored_list = scoring_module(name='glove', model_loc='/content/drive/My Drive/Movielensdata/ml25m/glove/glv', data_loc='/content/drive/My Drive/Movielensdata/ml25m/scoring_input', dict_loc='/content/drive/My Drive/Movielensdata/ml25m/tfidf/dict', matrix_location ='/content/drive/My Drive/Movielensdata/ml25m/cooccur/matrix' )"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vuSkxUwYf8J",
        "colab_type": "text"
      },
      "source": [
        "Change name, output location or all models and dict_loc for LSI and TF-IDF "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_4LF_ieeXvW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_module(reviews=True, name='hashing',tfidf_features =100,max_seq_length=100,location='/content/drive/My Drive/Movielensdata/ml25m/imdb/data',output_location='/content/drive/My Drive/Movielensdata/ml25m/imdb/processed_data/train_sent', dict_location='/content/drive/My Drive/Movielensdata/ml25m/imdb/dict/tfidf_dict', train_data_loc = '/content/drive/My Drive/Movielensdata/ml25m/imdb/processed_data/train_sent', test_data_loc = '/content/drive/My Drive/Movielensdata/ml25m/imdb/processed_data/test_sent', val_data_loc = '/content/drive/My Drive/Movielensdata/ml25m/imdb/processed_data/val_sent', Y_train_loc ='/content/drive/My Drive/Movielensdata/ml25m/imdb/Y/Y_train', Y_test_loc = '/content/drive/My Drive/Movielensdata/ml25m/imdb/Y/Y_test', Y_val_loc = '/content/drive/My Drive/Movielensdata/ml25m/imdb/Y/Y_val')"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaH5acqZYYI5",
        "colab_type": "text"
      },
      "source": [
        "Change name, Data_location and model_save_location for all models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EsUKDzuXFFL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_model(name='hashing',Data_location='/content/drive/My Drive/Movielensdata/ml25m/imdb/processed_data/train_sent',model_save_location='/content/drive/My Drive/Movielensdata/ml25m/imdb/trained_model', matrix_location='/content/drive/My Drive/Movielensdata/ml25m/imdb/cooccur_matrix',cooccur_features=1500, vector_dims=100,Sg=1,size_window=3,topics=10,mini_count=1,num_workers=3,max_num_epochs = 10,alpha = 0.025,min_alpha=0.00025,dm=1)"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajnRU9EfWhmP",
        "colab_type": "text"
      },
      "source": [
        "Change name for all models and dict_location for LSI and TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIR9yYbeb_ES",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scored_test= scoring_module(name='hashing', model_loc='/content/drive/My Drive/Movielensdata/ml25m/imdb/trained_model', data_loc='/content/drive/My Drive/Movielensdata/ml25m/imdb/processed_data/test_sent', dict_loc='/content/drive/My Drive/Movielensdata/ml25m/imdb/dict/tfidf_dict', matrix_location ='/content/drive/My Drive/Movielensdata/ml25m/imdb/cooccur_matrix' )"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BbXZZXvcU1H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scored_val = scoring_module(name='hashing', model_loc='/content/drive/My Drive/Movielensdata/ml25m/imdb/trained_model', data_loc='/content/drive/My Drive/Movielensdata/ml25m/imdb/processed_data/val_sent', dict_loc='/content/drive/My Drive/Movielensdata/ml25m/imdb/dict/tfidf_dict', matrix_location ='/content/drive/My Drive/Movielensdata/ml25m/imdb/cooccur_matrix' )"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWEvzm4ncfeP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert word-level embeddings to sentence-level and return tensors\n",
        "test_tensor = sent_embedding(name='hashing',vec_dims=100,score_list=scored_test)\n",
        "val_tensor = sent_embedding(name='hashing',vec_dims=100,score_list=scored_val)\n",
        "\n",
        "# Loads the Y-labels for test and val data\n",
        "Y_test = load_object('/content/drive/My Drive/Movielensdata/ml25m/imdb/Y/Y_test')\n",
        "Y_val = load_object('/content/drive/My Drive/Movielensdata/ml25m/imdb/Y/Y_val')"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Krs-h734cvcF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "outputId": "6fb70cdf-4a97-470d-bf6d-cb072b447b6d"
      },
      "source": [
        "sentiment_scores, accuracy = Model_build(X=test_tensor, Y=Y_test, X_predict=val_tensor, Y_predict=Y_val,accuracy_save_location='/content/drive/My Drive/Movielensdata/ml25m/imdb/accuracy/acc_hashing',sent_save_location='/content/drive/My Drive/Movielensdata/ml25m/imdb/sent_scores/sent_hashing',batches_size=100)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.6521 - accuracy: 0.6216\n",
            "Epoch 2/10\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.6074 - accuracy: 0.6757\n",
            "Epoch 3/10\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.5989 - accuracy: 0.6879\n",
            "Epoch 4/10\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.5960 - accuracy: 0.6856\n",
            "Epoch 5/10\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.5884 - accuracy: 0.7005\n",
            "Epoch 6/10\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.5863 - accuracy: 0.6975\n",
            "Epoch 7/10\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.5823 - accuracy: 0.7027\n",
            "Epoch 8/10\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.5765 - accuracy: 0.7049\n",
            "Epoch 9/10\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.5737 - accuracy: 0.7071\n",
            "Epoch 10/10\n",
            "150/150 [==============================] - 0s 2ms/step - loss: 0.5715 - accuracy: 0.7108\n",
            "INFO:tensorflow:Assets written to: /content/drive/My Drive/Movielensdata/ml25m/imdb/model/assets\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 0.5747 - accuracy: 0.6983\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJiPaOzUeI9P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "9751f183-aa0b-4475-91c7-a8805b22220f"
      },
      "source": [
        "sentiment_scores[0:10]"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.6879281 ],\n",
              "       [0.5491156 ],\n",
              "       [0.6817043 ],\n",
              "       [0.64840263],\n",
              "       [0.03520676],\n",
              "       [0.6234844 ],\n",
              "       [0.5138362 ],\n",
              "       [0.7188653 ],\n",
              "       [0.5233281 ],\n",
              "       [0.7415265 ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkOnPLL1eKZR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "be58055d-e979-4c34-f188-95bdc6274b2b"
      },
      "source": [
        "Y_val[0:10]"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, 1, 0, 0, 0, 1, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7xAYo8SOjxp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4b40c320-3e92-46fe-8bf4-06b74f36a2c0"
      },
      "source": [
        "accuracy"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6983000040054321"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1P1gaj9_Ttlu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 79,
      "outputs": []
    }
  ]
}