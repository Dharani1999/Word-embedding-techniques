{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Code_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1wgpB7rU7rrk4BnoapdXtGyFmepqlRw6K",
      "authorship_tag": "ABX9TyOMyCVEKE4xfUt3cSSVx2e/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dharani1999/Word-embedding-techniques/blob/master/Code_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTPPDaIkL-DE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import gensim\n",
        "from gensim import corpora, models, similarities\n",
        "from gensim.models import Word2Vec, TfidfModel, LsiModel\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from gensim.models.fasttext import FastText\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4XGZScCdZgS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "outputId": "46e7215f-93d4-43de-a630-e1cd846b3a79"
      },
      "source": [
        "#!pip install git+https://github.com/huggingface/transformers\n",
        "!pip install glove_python\n",
        "from glove import Corpus, Glove# creating a corpus object"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting glove_python\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3e/79/7e7e548dd9dcb741935d031117f4bed133276c2a047aadad42f1552d1771/glove_python-0.1.0.tar.gz (263kB)\n",
            "\r\u001b[K     |█▎                              | 10kB 17.2MB/s eta 0:00:01\r\u001b[K     |██▌                             | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |███▊                            | 30kB 2.2MB/s eta 0:00:01\r\u001b[K     |█████                           | 40kB 2.5MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 51kB 2.1MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 61kB 2.3MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 71kB 2.5MB/s eta 0:00:01\r\u001b[K     |██████████                      | 81kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 92kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 102kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 112kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 122kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 133kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 143kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 153kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 163kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 174kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 184kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 194kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 204kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 215kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 225kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 235kB 2.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 245kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 256kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 266kB 2.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from glove_python) (1.18.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from glove_python) (1.4.1)\n",
            "Building wheels for collected packages: glove-python\n",
            "  Building wheel for glove-python (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for glove-python: filename=glove_python-0.1.0-cp36-cp36m-linux_x86_64.whl size=700234 sha256=cf96ac10f10f62b5ff0c7c49b4537111ddd266eb2a2664fc93dd96c41f93991c\n",
            "  Stored in directory: /root/.cache/pip/wheels/88/4b/6d/10c0d2ad32c9d9d68beec9694a6f0b6e83ab1662a90a089a4b\n",
            "Successfully built glove-python\n",
            "Installing collected packages: glove-python\n",
            "Successfully installed glove-python-0.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EslXGch8IZgd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_module(name, location, output_location, dict_location, max_seq_length=100):\n",
        "  dataset1 = pd.read_csv(location)\n",
        "  dataset = dataset1.iloc[0:1000,:]\n",
        "\n",
        "  if name == 'word2vec':\n",
        "    data_corpus, users_total = data_word2vec(dataset,max_seq_length)\n",
        "  elif name == 'doc2vec':\n",
        "    data_corpus, users_total = data_doc2vec(dataset,max_seq_length)\n",
        "  elif name == 'lsi':\n",
        "    data_corpus, users_total = data_lsi(dataset,max_seq_length,dict_loc=dict_location)\n",
        "  elif name == 'tfidf':\n",
        "    data_corpus, users_total = data_lsi(dataset,max_seq_length,dict_loc=dict_location)\n",
        "  elif name == 'glove':\n",
        "    data_corpus, users_total = data_word2vec(dataset,max_seq_length)\n",
        "  elif name == 'hashing':\n",
        "    data_corpus, users_total = data_hashing(dataset,max_seq_length)\n",
        "  elif name == 'cooccur':\n",
        "    data_corpus, users_total = data_hashing(dataset,max_seq_length)\n",
        "  #elif name == 'fasttext':\n",
        "   # data_corpus, users_total = data_word2vec(dataset,max_seq_length)\n",
        "  #data_corpus.save('/content/drive/My Drive/Movielensdata/ml25m/data_corpus')\n",
        "  #print(data_corpus)\n",
        "  save_object(obj=data_corpus, filename=output_location)\n",
        "  #return data_corpus, users_total\n",
        "\n",
        "def data_word2vec(dataset,max_seq_length):\n",
        "  dataset.sort_values(by=['userId','timestamp'],inplace=True)\n",
        "  user_total = len(dataset['userId'].unique())\n",
        "  \n",
        "  #Selecting the most recent movies rated by each user and padding if necessary\n",
        "  movie_list = []\n",
        "  for i in range(user_total):\n",
        "    list1 = []\n",
        "    list1 = dataset.loc[dataset['userId'] ==(i+1),['movieId']]['movieId'].tolist()\n",
        "    if len(list1)>max_seq_length:\n",
        "      list1 = list1[(len(list1)-max_seq_length):]\n",
        "    elif len(list1)<max_seq_length:\n",
        "      list1 = list1+[0 for j in range((max_seq_length-len(list1)))]\n",
        "      #for j in range((max_seq_length-len(list1))):\n",
        "       # list1.append(0)\n",
        "    movie_list.append(list1)\n",
        "  \n",
        "  #Selecting the most recent ratings rated by each user and padding if necessary\n",
        "  rating_list =[]\n",
        "  for i in range(user_total):\n",
        "    list2 = []\n",
        "    list2 = dataset.loc[dataset['userId'] ==(i+1),['rating']]['rating'].tolist()\n",
        "    if len(list2)>max_seq_length:\n",
        "      list2 = list2[(len(list2)-max_seq_length):]\n",
        "    elif len(list2)<max_seq_length:\n",
        "      list2 = list2+[0 for j in range((max_seq_length-len(list2)))]\n",
        "      #for j in range((max_seq_length-len(list2))):\n",
        "       # list2.append(0)\n",
        "    rating_list.append(list2)\n",
        "  \n",
        "  #Creating user_id level transpose matrices\n",
        "  movies_transpose = pd.DataFrame(data=movie_list,index=[i+1 for i in range(user_total)])\n",
        "  movies_transpose.index.names = ['userId']\n",
        "  #print(movies_transpose)\n",
        "\n",
        "  ratings_transpose = pd.DataFrame(data=rating_list,index=[i+1 for i in range(user_total)])\n",
        "  ratings_transpose.index.names = ['userId']\n",
        "  #print(ratings_transpose)\n",
        "\n",
        "  # Select features from original dataset to form a new dataframe \n",
        "  df1 = movies_transpose.iloc[:]# For each row, combine all the columns into one column\n",
        "  df2 = df1.apply(lambda x: ','.join(x.astype(str)), axis=1)# Store them in a pandas dataframe\n",
        "  df_clean = pd.DataFrame({'clean': df2})# Create the list of list format of the custom corpus for gensim modeling \n",
        "  sent = [row.split(',') for row in df_clean['clean']]\n",
        "\n",
        "  return sent, user_total\n",
        "\n",
        "def data_doc2vec(dataset,max_seq_length):\n",
        "  Sent, user_total = data_word2vec(dataset,max_seq_length)\n",
        "  tagged_data = []\n",
        "  tags = []\n",
        "  \n",
        "  for i in range(user_total):\n",
        "    tagged_data = tagged_data + [TaggedDocument(words=Sent[i], tags=[str(i)])]\n",
        "\n",
        "  return tagged_data, user_total\n",
        "\n",
        "def data_lsi(dataset,max_seq_length,dict_loc):\n",
        "  Sent, user_total = data_word2vec(dataset,max_seq_length)\n",
        "  dictionary = corpora.Dictionary(Sent)\n",
        "  #print(dictionary.token2id)\n",
        "  corpus = [dictionary.doc2bow(text) for text in Sent]\n",
        "  dictionary.save(dict_loc)\n",
        "  #corpus = np.array([[(id, freq) for id, freq in cp] for cp in corp])\n",
        "  #corpus = gensim.matutils.Dense2Corpus(np.array(Sent),documents_columns=False)\n",
        "\n",
        "  return corpus, user_total\n",
        "\n",
        "def data_hashing(dataset,max_seq_length):\n",
        "  Sent, user_total = data_word2vec(dataset,max_seq_length)\n",
        "  corpus = [str(str(doc)[1:-1]) for doc in Sent]\n",
        "  return corpus, user_total"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nrx8P2P32kT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_object(obj, filename):\n",
        "    with open(filename, 'wb') as output:  # Overwrites any existing file.\n",
        "        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "def load_object(filename):\n",
        "    with open(filename, 'rb') as input:\n",
        "        pickle_object = pickle.load(input)\n",
        "    return  pickle_object"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kJytoDNK3E0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def embedding_model(name,Data_location,model_save_location,matrix_location, vector_dims=10,Sg=1,size_window=3,topics=10,mini_count=1,num_workers=3,max_num_epochs = 100,alpha = 0.025,min_alpha=0.00025,dm=1, maxi_features=None):\n",
        "  Data = load_object(Data_location)\n",
        "  #print(Data)\n",
        "  if name == 'word2vec':\n",
        "    word2vec(input_data=Data, save_loc=model_save_location, vec_dims=vector_dims, SG=Sg, size_of_window=size_window, minimum_count=mini_count, no_workers=num_workers)\n",
        "    #voc = model1.wv\n",
        "    #words = list(model1.wv.vocab)\n",
        "    #vectors = model1[model1.wv.vocab]\n",
        "  elif name == 'doc2vec':\n",
        "    doc2vec(input_data=Data, save_loc=model_save_location, vec_dims=vector_dims, alpha_=alpha, size_of_window=size_window, no_workers=num_workers, max_epochs=max_num_epochs,min_alpha_=min_alpha, minimum_count=mini_count, dms=dm)\n",
        "  elif name == 'lsi':\n",
        "    lsi(input_data=Data, save_loc=model_save_location, total_topics=topics)\n",
        "  elif name == 'tfidf':\n",
        "    tfidf(input_data=Data,save_loc=model_save_location)\n",
        "  elif name == 'glove':\n",
        "    glove_model(input_data=Data, vec_dims=vector_dims, size_of_window=size_window, save_loc=model_save_location, num_epochs=max_num_epochs, alpha_=0.05, num_threads=4)\n",
        "  elif name == 'hashing':\n",
        "    hashing(input_data=Data, vec_dims=vector_dims, save_loc=model_save_location)\n",
        "  elif name == 'cooccur':\n",
        "    co_occur(input_data=Data, maximum_features=maxi_features, save_loc=model_save_location, matrix_loc = matrix_location)\n",
        "  #elif name == 'fasttext':\n",
        "   # fast_text(input_data=Data, save_loc=model_save_location, vec_dims=vector_dims, SG=Sg, size_of_window=size_window, minimum_count=mini_count, no_workers=num_workers, alpha_=0.025)\n",
        "\n",
        "def word2vec(input_data,save_loc,vec_dims,SG,size_of_window,minimum_count,no_workers):\n",
        "  model = Word2Vec(input_data,min_count=minimum_count,size= vec_dims,workers=no_workers, window =size_of_window, sg = SG)\n",
        "  model.save(save_loc)\n",
        "\n",
        "def doc2vec(input_data,save_loc,vec_dims,alpha_,size_of_window,min_alpha_,minimum_count,dms,no_workers,max_epochs):\n",
        "  model = Doc2Vec(size=vec_dims,\n",
        "                alpha=alpha_, \n",
        "                min_alpha=min_alpha_,\n",
        "                window = size_of_window,\n",
        "                min_count=minimum_count,\n",
        "                dm =dms)\n",
        "  model.build_vocab(input_data)\n",
        "\n",
        "  for epoch in range(max_epochs):\n",
        "    #print('iteration {0}'.format(epoch))\n",
        "    model.train(input_data, total_examples=model.corpus_count, epochs=model.iter)\n",
        "    # decrease the learning rate\n",
        "    model.alpha -= 0.0002\n",
        "    # fix the learning rate, no decay\n",
        "    model.min_alpha = model.alpha\n",
        "  model.save(save_loc)\n",
        "\n",
        "def lsi(input_data,save_loc,total_topics):\n",
        "  model = models.LsiModel(corpus=input_data, num_topics=total_topics)\n",
        "  index = similarities.MatrixSimilarity(model[input_data])\n",
        "  lsi_data = model[input_data]\n",
        "  lsi_topics = model.print_topics()\n",
        "  #for topic in lsi_topics:\n",
        "    #print(topic)\n",
        "  model.save(save_loc)\n",
        "\n",
        "def tfidf(input_data,save_loc):\n",
        "  model = models.TfidfModel(corpus=input_data)\n",
        "  tfidf_data = model[input_data]\n",
        "\n",
        "  tfidf_token= np.zeros((len(tfidf_data), 350), dtype=np.float64)\n",
        "  tfidf_vals= np.zeros((len(tfidf_data), 350), dtype=np.float64)\n",
        " \n",
        "  for i in range(len(input_data)):\n",
        "    for k in range(len(list(tfidf_data)[i])):\n",
        "      tfidf_token[i][k]=(list(tfidf_data))[i][k][0]\n",
        "      tfidf_vals[i][k]=(list(tfidf_data))[i][k][1]\n",
        "  tfidf_list=list(tfidf_data)\n",
        "  #print(list(tfidf_data))\n",
        "  model.save(save_loc)\n",
        "\n",
        "def glove_model(input_data,vec_dims,size_of_window,save_loc,alpha_=0.05,num_epochs=30, num_threads=4):\n",
        "  #importing the glove library\n",
        "  corpus = Corpus() #training the corpus to generate the co occurence matrix which is used in GloVe\n",
        "  corpus.fit(input_data, window=size_of_window)#creating a Glove object which will use the matrix created in the above lines to create embeddings\n",
        "  #We can set the learning rate as it uses Gradient Descent and number of components\n",
        "  glove = Glove(no_components=vec_dims, learning_rate=alpha_) \n",
        "  glove.fit(corpus.matrix, epochs=num_epochs, no_threads=4, verbose=True)\n",
        "  glove.add_dictionary(corpus.dictionary)\n",
        "  glove.save(save_loc)\n",
        "\n",
        "def fast_text(input_data,save_loc,vec_dims,SG,size_of_window,minimum_count,no_workers,alpha_=0.025):\n",
        "  model = FastText(min_count=minimum_count, alpha=alpha_, size= vec_dims, workers=no_workers, window =size_of_window)\n",
        "  model.build_vocab(input_data)\n",
        "  model.train(input_data, epochs=model.epochs, total_examples=model.corpus_count, total_words=model.corpus_total_words)\n",
        "  model.save(save_loc)\n",
        "\n",
        "def hashing(input_data,vec_dims,save_loc):\n",
        "  model = HashingVectorizer(n_features=vec_dims)\n",
        "  model.transform(input_data)\n",
        "  #vectors = model.toarray()\n",
        "  #vocab = model.get_feature_names()\n",
        "  save_object(obj=model, filename=save_loc)\n",
        "\n",
        "def co_occur(input_data, save_loc, matrix_loc, maximum_features):\n",
        "  model = CountVectorizer(ngram_range=(1,1),max_features=maximum_features, token_pattern= r\"(?u)\\b\\w+\\b\")\n",
        "  X = model.fit_transform(input_data)\n",
        "  Xc = (X.T * X)\n",
        "  Xc.setdiag(0)\n",
        "  #cooccur = Xc.todense()\n",
        "  names = model.get_feature_names() # This are the entity names (i.e. keywords)\n",
        "  save_object(obj=names, filename='/content/drive/My Drive/Movielensdata/ml25m/cooccur/vocab')\n",
        "  df = pd.DataFrame(data = Xc.toarray(), columns = names, index = names)\n",
        "  save_object(obj=model, filename=save_loc)\n",
        "  save_object(obj=df, filename=matrix_loc)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXlM6wLvQVIW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scoring_module(name,model_loc,data,dict_loc,matrix_location):\n",
        "  if name == 'word2vec':\n",
        "    scores_list = score_word2vec(model_location=model_loc,input_data=data)\n",
        "  elif name == 'doc2vec':\n",
        "    scores_list = score_doc2vec(model_location=model_loc,input_data=data)\n",
        "  elif name == 'lsi':\n",
        "    scores_list = score_lsi(model_location=model_loc,input_data=data, dict_location=dict_loc)\n",
        "  elif name == 'tfidf':\n",
        "    scores_list = score_tfidf(model_location=model_loc,input_data=data, dict_location=dict_loc)\n",
        "  elif name == 'glove':\n",
        "    scores_list = score_glove(model_location=model_loc, input_data=data)\n",
        "  elif name == 'hashing':\n",
        "    scores_list = score_hashing(model_location=model_loc, input_data=data)\n",
        "  elif name == 'cooccur':\n",
        "    scores_list = score_cooccur(matrix_loc=matrix_location, input_data=data)\n",
        "  #elif name == 'fasttext':\n",
        "   # scores_list = score_fasttext(model_location=model_loc, input_data=data)\n",
        "  \n",
        "  return scores_list\n",
        "\n",
        "def score_word2vec(model_location,input_data):\n",
        "  model = Word2Vec.load(model_location)\n",
        "  scored_data = [model.wv[text] for text in input_data]\n",
        "  return scored_data\n",
        "\n",
        "def score_doc2vec(model_location,input_data):\n",
        "  model = Doc2Vec.load(model_location)\n",
        "  scored_data = [model.infer_vector(text) for text in input_data]\n",
        "  return scored_data\n",
        "\n",
        "def score_glove(model_location,input_data):\n",
        "  model = Glove.load(model_location)\n",
        "  scored_data = [model.word_vectors[model.dictionary[text]] for doc in input_data for text in doc]\n",
        "  #model.word_vectors[]\n",
        "  return scored_data\n",
        "\n",
        "def score_fasttext(model_location, input_data):\n",
        "  model = FastText(model_location)\n",
        "  scored_data = [model[text] for text in input_data]\n",
        "  return scored_data\n",
        "\n",
        "def score_lsi(model_location, input_data, dict_location):\n",
        "  #model = LsiModel.load(model_location)\n",
        "  dictionary = corpora.Dictionary.load(dict_location)\n",
        "  scored_data = [model[dictionary.doc2bow(text)] for text in input_data]\n",
        "  return scored_data\n",
        "\n",
        "def score_tfidf(model_location, input_data, dict_location):\n",
        "  #model = TfidfModel.load(model_location)\n",
        "  dictionary = corpora.Dictionary.load(dict_location)\n",
        "  scored_data = [model[dictionary.doc2bow(text)] for text in input_data]\n",
        "  return scored_data\n",
        "\n",
        "def score_hashing(model_location, input_data):\n",
        "  corpus = [str(str(doc)[1:-1]) for doc in input_data]\n",
        "  model = load_object(model_location)\n",
        "  X = model.transform(corpus)\n",
        "  Doc_Term_Matrix = pd.DataFrame(X.toarray())\n",
        "  Doc_Term_Matrix\n",
        "  return Doc_Term_Matrix\n",
        "\n",
        "def score_cooccur(matrix_loc, input_data):\n",
        "  cooccur_matrix = load_object(matrix_loc)\n",
        "  #print(cooccur_matrix)\n",
        "  scored_data = [cooccur_matrix.loc[text].tolist() for doc in input_data for text in doc]\n",
        "  return scored_data"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTON8_HeWKAV",
        "colab_type": "text"
      },
      "source": [
        "1) Name can be 'word2vec' or 'doc2vec' or 'lsi' or 'tfidf' or 'glove'.\n",
        "\n",
        "2) location is the path to input .csv file\n",
        "\n",
        "3) output_location is the path to save the processed data\n",
        "\n",
        "4) dict_location is the path to dictionary of LSI or TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajnRU9EfWhmP",
        "colab_type": "text"
      },
      "source": [
        "Change name, output_location for all models and dict_location for LSI and TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXdIQWmIGFOG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "outputId": "f48b8d8f-dd45-4518-defb-369bce67da67"
      },
      "source": [
        "data_module(name='cooccur',max_seq_length=100,location='/content/drive/My Drive/Movielensdata/ml25m/ratings.csv',output_location='/content/drive/My Drive/Movielensdata/ml25m/cooccur/data', dict_location='/content/drive/My Drive/Movielensdata/ml25m/tfidf/dict')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaH5acqZYYI5",
        "colab_type": "text"
      },
      "source": [
        "Change name, Data_location and model_save_location for all models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iWoBWvRZwSD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_model(name='cooccur',Data_location='/content/drive/My Drive/Movielensdata/ml25m/cooccur/data',model_save_location='/content/drive/My Drive/Movielensdata/ml25m/cooccur/co_occur', matrix_location='/content/drive/My Drive/Movielensdata/ml25m/cooccur/matrix', vector_dims=10,Sg=1,size_window=3,topics=10,mini_count=1,num_workers=3,max_num_epochs = 10,alpha = 0.025,min_alpha=0.00025,dm=1,maxi_features=None)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WqzalHzc_lp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scoring_input = [['899', '2161', '3949', '5878', '1175', '1237', '8154', '2843', '7365', '4422', '6016'],['1080', '3114', '3671', '2791', '1288', '1', '541', '2692', '7323', '8014', '6370', '4703', '5147']]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vuSkxUwYf8J",
        "colab_type": "text"
      },
      "source": [
        "Change name, model_loc or all models and dict_loc for LSI and TF-IDF "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ry_oK4hcUcJb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scored_list = scoring_module(name='cooccur', model_loc='/content/drive/My Drive/Movielensdata/ml25m/cooccur/co_occur', data=scoring_input, dict_loc='/content/drive/My Drive/Movielensdata/ml25m/tfidf/dict', matrix_location ='/content/drive/My Drive/Movielensdata/ml25m/cooccur/matrix' )"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEHVPbZmaO3M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "9136bb3f-fce9-49a7-db70-41acce8b55de"
      },
      "source": [
        "print(scored_list[0])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[30, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Io4aqWLneCm1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(scored_list)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwSmRftsWnwp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "bca00d1b-78c2-418f-ad23-fd80dbfd2deb"
      },
      "source": [
        "!pip install sentencepiece\n",
        "!git clone https://github.com/google-research/bert\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import nltk\n",
        "import random\n",
        "import logging\n",
        "import tensorflow as tf\n",
        "import tensorboard as tb\n",
        "tf.gfile = tf.io.gfile\n",
        "tf.io.gfile = tb.compat.tensorflow_stub.io.gfile\n",
        "import sentencepiece as spm\n",
        "\n",
        "from glob import glob\n",
        "from google.colab import auth, drive\n",
        "from tensorflow.keras.utils import Progbar\n",
        "\n",
        "sys.path.append(\"bert\")\n",
        "\n",
        "from bert import modeling, tokenization\n",
        "#from bert.run_pretraining import input_fn_builder, model_fn_builder"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 2.9MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.91\n",
            "Cloning into 'bert'...\n",
            "remote: Enumerating objects: 340, done.\u001b[K\n",
            "remote: Total 340 (delta 0), reused 0 (delta 0), pack-reused 340\u001b[K\n",
            "Receiving objects: 100% (340/340), 317.20 KiB | 8.35 MiB/s, done.\n",
            "Resolving deltas: 100% (185/185), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6C8oUv-OIYx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bert_vocab = load_object('/content/drive/My Drive/Movielensdata/ml25m/cooccur/vocab')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MX27c1j0Vzph",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "4b2ccbe6-c6f3-4259-81c7-0e0a7a626e9e"
      },
      "source": [
        "print(bert_vocab)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['0', '1', '102407', '102686', '102716', '103141', '103384', '1036', '103810', '104241', '105213', '106489', '107348', '1080', '1088', '109578', '111113', '111443', '111659', '112138', '112171', '1136', '1175', '1196', '1197', '1198', '1200', '1201', '1210', '1214', '1217', '1220', '122896', '122906', '1237', '1246', '1250', '1257', '1260', '1265', '1270', '1271', '1275', '1278', '1287', '1288', '1291', '1293', '129354', '1302', '130634', '135143', '135536', '135569', '1356', '136020', '136864', '1376', '1393', '139644', '140956', '143385', '1465', '149406', '1527', '1580', '158238', '1584', '158966', '159093', '1610', '1653', '166635', '1672', '1674', '168612', '1693', '171763', '1732', '176101', '176371', '179819', '1873', '187541', '187593', '1907', '1923', '1960', '2011', '2012', '2019', '2028', '2068', '2083', '2115', '2138', '2139', '2150', '2161', '2268', '2294', '2324', '2351', '2355', '2359', '2501', '2571', '2573', '260', '261', '2632', '2692', '2716', '27193', '27266', '2745', '2761', '27660', '27721', '2791', '2843', '2918', '2951', '296', '2985', '2987', '2993', '3033', '3039', '306', '307', '30749', '30848', '3098', '3105', '3114', '31221', '3148', '3175', '318', '31923', '31956', '32591', '33166', '33493', '3360', '33660', '33794', '3396', '34048', '34405', '3448', '349', '3569', '35836', '3624', '364', '36527', '3671', '3681', '3751', '3793', '38038', '3827', '39435', '3949', '3968', '3994', '4016', '4023', '4103', '4144', '4262', '4306', '4308', '4325', '44191', '44193', '4422', '4448', '44665', '4535', '45431', '45666', '46335', '4703', '4709', '4720', '480', '4816', '4874', '4886', '4958', '4963', '4973', '4974', '49822', '4993', '4995', '5010', '5103', '5110', '5147', '5219', '524', '5269', '5299', '53129', '5313', '534', '541', '5418', '5444', '54648', '55805', '56156', '5618', '5684', '57528', '5767', '5878', '588', '589', '5900', '5903', '5912', '59501', '5952', '5955', '59900', '6016', '61248', '6156', '62', '62999', '6311', '6370', '6377', '64030', '6539', '6548', '6550', '65514', '6565', '665', '6711', '67923', '6863', '6874', '6879', '68793', '68952', '6947', '6954', '70336', '7075', '7090', '71156', '7143', '71464', '7153', '7162', '7209', '7234', '7318', '7323', '7324', '7327', '73323', '7361', '7365', '7373', '7438', '7451', '7569', '7624', '780', '7820', '79091', '79224', '7937', '7938', '7939', '7940', '8010', '8014', '81537', '8154', '81834', '82202', '8327', '83349', '8360', '8368', '8405', '85020', '858', '8636', '8641', '86644', '8665', '8685', '86911', '8729', '8786', '87876', '88125', '88405', '8861', '8873', '8874', '8914', '8958', '8961', '8970', '8973', '89753', '8983', '899', '90866', '914', '91974', '91976', '924', '94018', '94677', '953', '96079', '96861', '97225', '97306', '97923', '99112']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DRI6GIRV82Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ctrl_symbols = [\"[PAD]\",\"[UNK]\",\"[CLS]\",\"[SEP]\",\"[MASK]\"]\n",
        "bert_vocab = ctrl_symbols + bert_vocab"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFM-Dws-VaFV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "95d6a8cf-4d32-4d17-9bbb-755c684273f4"
      },
      "source": [
        "print(bert_vocab)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '0', '1', '102407', '102686', '102716', '103141', '103384', '1036', '103810', '104241', '105213', '106489', '107348', '1080', '1088', '109578', '111113', '111443', '111659', '112138', '112171', '1136', '1175', '1196', '1197', '1198', '1200', '1201', '1210', '1214', '1217', '1220', '122896', '122906', '1237', '1246', '1250', '1257', '1260', '1265', '1270', '1271', '1275', '1278', '1287', '1288', '1291', '1293', '129354', '1302', '130634', '135143', '135536', '135569', '1356', '136020', '136864', '1376', '1393', '139644', '140956', '143385', '1465', '149406', '1527', '1580', '158238', '1584', '158966', '159093', '1610', '1653', '166635', '1672', '1674', '168612', '1693', '171763', '1732', '176101', '176371', '179819', '1873', '187541', '187593', '1907', '1923', '1960', '2011', '2012', '2019', '2028', '2068', '2083', '2115', '2138', '2139', '2150', '2161', '2268', '2294', '2324', '2351', '2355', '2359', '2501', '2571', '2573', '260', '261', '2632', '2692', '2716', '27193', '27266', '2745', '2761', '27660', '27721', '2791', '2843', '2918', '2951', '296', '2985', '2987', '2993', '3033', '3039', '306', '307', '30749', '30848', '3098', '3105', '3114', '31221', '3148', '3175', '318', '31923', '31956', '32591', '33166', '33493', '3360', '33660', '33794', '3396', '34048', '34405', '3448', '349', '3569', '35836', '3624', '364', '36527', '3671', '3681', '3751', '3793', '38038', '3827', '39435', '3949', '3968', '3994', '4016', '4023', '4103', '4144', '4262', '4306', '4308', '4325', '44191', '44193', '4422', '4448', '44665', '4535', '45431', '45666', '46335', '4703', '4709', '4720', '480', '4816', '4874', '4886', '4958', '4963', '4973', '4974', '49822', '4993', '4995', '5010', '5103', '5110', '5147', '5219', '524', '5269', '5299', '53129', '5313', '534', '541', '5418', '5444', '54648', '55805', '56156', '5618', '5684', '57528', '5767', '5878', '588', '589', '5900', '5903', '5912', '59501', '5952', '5955', '59900', '6016', '61248', '6156', '62', '62999', '6311', '6370', '6377', '64030', '6539', '6548', '6550', '65514', '6565', '665', '6711', '67923', '6863', '6874', '6879', '68793', '68952', '6947', '6954', '70336', '7075', '7090', '71156', '7143', '71464', '7153', '7162', '7209', '7234', '7318', '7323', '7324', '7327', '73323', '7361', '7365', '7373', '7438', '7451', '7569', '7624', '780', '7820', '79091', '79224', '7937', '7938', '7939', '7940', '8010', '8014', '81537', '8154', '81834', '82202', '8327', '83349', '8360', '8368', '8405', '85020', '858', '8636', '8641', '86644', '8665', '8685', '86911', '8729', '8786', '87876', '88125', '88405', '8861', '8873', '8874', '8914', '8958', '8961', '8970', '8973', '89753', '8983', '899', '90866', '914', '91974', '91976', '924', '94018', '94677', '953', '96079', '96861', '97225', '97306', '97923', '99112']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLnA43qPWEaQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VOC_FNAME = \"vocab.txt\"\n",
        "\n",
        "with open(VOC_FNAME, \"w\") as fo:\n",
        "  for token in bert_vocab:\n",
        "    fo.write(token+\"\\n\")"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmpkX5dqWkvB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "testcase = \"'5952', '2012', '0'\""
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7k-rn6lWX443",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "cf1d5e3a-d515-4725-d041-bbbf0e6be966"
      },
      "source": [
        "bert_tokenizer = tokenization.FullTokenizer(VOC_FNAME)\n",
        "bert_tokenizer.tokenize(testcase)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[UNK]',\n",
              " '5952',\n",
              " '[UNK]',\n",
              " '[UNK]',\n",
              " '[UNK]',\n",
              " '2012',\n",
              " '[UNK]',\n",
              " '[UNK]',\n",
              " '[UNK]',\n",
              " '0',\n",
              " '[UNK]']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEBqGJlnYrmZ",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "MAX_SEQ_LENGTH = 128 #@param {type:\"integer\"}\n",
        "MASKED_LM_PROB = 0.15 #@param\n",
        "MAX_PREDICTIONS = 20 #@param {type:\"integer\"}\n",
        "DO_LOWER_CASE = True #@param {type:\"boolean\"}\n",
        "PROCESSES = 2 #@param {type:\"integer\"}\n",
        "PRETRAINING_DIR = \"pretraining_data\" #@param {type:\"string\"}"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zL838s3XYimz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.io.gfile.makedirs(PRETRAINING_DIR)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45qP8fetYitW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MODEL_DIR = \"bert_model\" #@param {type:\"string\"}\n",
        "tf.io.gfile.makedirs(MODEL_DIR)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJDI4LFWYiqM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use this for BERT-base\n",
        "VOC_SIZE = 300\n",
        "bert_base_config = {\n",
        "  \"attention_probs_dropout_prob\": 0.1, \n",
        "  \"directionality\": \"bidi\", \n",
        "  \"hidden_act\": \"gelu\", \n",
        "  \"hidden_dropout_prob\": 0.1, \n",
        "  \"hidden_size\": 768, \n",
        "  \"initializer_range\": 0.02, \n",
        "  \"intermediate_size\": 3072, \n",
        "  \"max_position_embeddings\": 512, \n",
        "  \"num_attention_heads\": 12, \n",
        "  \"num_hidden_layers\": 12, \n",
        "  \"pooler_fc_size\": 768, \n",
        "  \"pooler_num_attention_heads\": 12, \n",
        "  \"pooler_num_fc_layers\": 3, \n",
        "  \"pooler_size_per_head\": 128, \n",
        "  \"pooler_type\": \"first_token_transform\", \n",
        "  \"type_vocab_size\": 2, \n",
        "  \"vocab_size\": VOC_SIZE\n",
        "}\n",
        "\n",
        "with open(\"{}/bert_config.json\".format(MODEL_DIR), \"w\") as fo:\n",
        "  json.dump(bert_base_config, fo, indent=2)\n",
        "  \n",
        "with open(\"{}/{}\".format(MODEL_DIR, VOC_FNAME), \"w\") as fo:\n",
        "  for token in bert_vocab:\n",
        "    fo.write(token+\"\\n\")"
      ],
      "execution_count": 25,
      "outputs": []
    }
  ]
}