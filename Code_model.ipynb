{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Code_model.ipynb",
      "provenance": [],
      "mount_file_id": "1wgpB7rU7rrk4BnoapdXtGyFmepqlRw6K",
      "authorship_tag": "ABX9TyMAsK6783517jcLiNA4Nr84",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dharani1999/Word-embedding-techniques/blob/master/Code_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTPPDaIkL-DE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot\n",
        "import gensim\n",
        "from gensim import corpora, models, similarities\n",
        "from gensim.models import Word2Vec, TfidfModel, LsiModel\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EslXGch8IZgd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_module(name,location,max_seq_length=100):\n",
        "  dataset1 = pd.read_csv(location)\n",
        "  dataset = dataset1.iloc[0:1000,:]\n",
        "\n",
        "  if name == 'word2vec':\n",
        "    data_corpus, users_total = data_word2vec(dataset,max_seq_length)\n",
        "  elif name == 'doc2vec':\n",
        "    data_corpus, users_total = data_doc2vec(dataset,max_seq_length)\n",
        "  elif name == 'lsi':\n",
        "    data_corpus, users_total = data_lsi(dataset,max_seq_length)\n",
        "  elif name == 'tfidf':\n",
        "    data_corpus, users_total = data_tfidf(dataset,max_seq_length)\n",
        "  \n",
        "  return data_corpus, users_total\n",
        "\n",
        "def data_word2vec(dataset,max_seq_length):\n",
        "  dataset.sort_values(by=['userId','timestamp'],inplace=True)\n",
        "  user_total = len(dataset['userId'].unique())\n",
        "  \n",
        "  #Selecting the most recent movies rated by each user and padding if necessary\n",
        "  movie_list = []\n",
        "  for i in range(user_total):\n",
        "    list1 = []\n",
        "    list1 = dataset.loc[dataset['userId'] ==(i+1),['movieId']]['movieId'].tolist()\n",
        "    if len(list1)>max_seq_length:\n",
        "      list1 = list1[(len(list1)-max_seq_length):]\n",
        "    elif len(list1)<max_seq_length:\n",
        "      for j in range((max_seq_length-len(list1))):\n",
        "        list1.append(0)\n",
        "    movie_list.append(list1)\n",
        "  \n",
        "  #Selecting the most recent ratings rated by each user and padding if necessary\n",
        "  rating_list =[]\n",
        "  for i in range(user_total):\n",
        "    list2 = []\n",
        "    list2 = dataset.loc[dataset['userId'] ==(i+1),['rating']]['rating'].tolist()\n",
        "    if len(list2)>max_seq_length:\n",
        "      list2 = list2[(len(list2)-max_seq_length):]\n",
        "    elif len(list2)<max_seq_length:\n",
        "      for j in range((max_seq_length-len(list2))):\n",
        "        list2.append(0)\n",
        "    rating_list.append(list2)\n",
        "  \n",
        "  #Creating user_id level transpose matrices\n",
        "  movies_transpose = pd.DataFrame(data=movie_list,index=[i+1 for i in range(user_total)])\n",
        "  movies_transpose.index.names = ['userId']\n",
        "  #print(movies_transpose)\n",
        "\n",
        "  ratings_transpose = pd.DataFrame(data=rating_list,index=[i+1 for i in range(user_total)])\n",
        "  ratings_transpose.index.names = ['userId']\n",
        "  #print(ratings_transpose)\n",
        "\n",
        "  # Select features from original dataset to form a new dataframe \n",
        "  df1 = movies_transpose.iloc[:]# For each row, combine all the columns into one column\n",
        "  df2 = df1.apply(lambda x: ','.join(x.astype(str)), axis=1)# Store them in a pandas dataframe\n",
        "  df_clean = pd.DataFrame({'clean': df2})# Create the list of list format of the custom corpus for gensim modeling \n",
        "  sent = [row.split(',') for row in df_clean['clean']]\n",
        "\n",
        "  return sent, user_total\n",
        "\n",
        "def data_doc2vec(dataset,max_seq_length):\n",
        "  Sent, user_total = data_word2vec(dataset,max_seq_length)\n",
        "  tagged_data = []\n",
        "  tags = []\n",
        "  \n",
        "  for i in range(user_total):\n",
        "    tagged_data = tagged_data + [TaggedDocument(words=Sent[i], tags=[str(i)])]\n",
        "\n",
        "  return tagged_data, user_total\n",
        "\n",
        "def data_lsi(dataset,max_seq_length):\n",
        "  Sent, user_total = data_word2vec(dataset,max_seq_length)\n",
        "  corpus = gensim.matutils.Dense2Corpus(np.array(Sent),documents_columns=False)\n",
        "  #print(list(corpus))\n",
        "\n",
        "  return corpus, user_total\n",
        "\n",
        "def data_tfidf(dataset,max_seq_length):\n",
        "  Sent, user_total = data_word2vec(dataset,max_seq_length)\n",
        "  corpus = gensim.matutils.Dense2Corpus(np.array(Sent),documents_columns=False)\n",
        "  \n",
        "  return corpus, user_total"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kJytoDNK3E0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def embedding_model(name,Data,model_save_location,vector_dims=50,Sg=1,size_window=3,topics=10,mini_count=1,num_workers=3,max_num_epochs = 100,alpha = 0.025,min_alpha=0.00025,dm=1):\n",
        "  if name == 'word2vec':\n",
        "    word2vec(input_data=Data,save_loc=model_save_location,vec_dims=vector_dims,SG=Sg,size_of_window=size_window,minimum_count=mini_count,no_workers=num_workers)\n",
        "    #voc = model1.wv\n",
        "    #words = list(model1.wv.vocab)\n",
        "    #vectors = model1[model1.wv.vocab]\n",
        "  elif name == 'doc2vec':\n",
        "    doc2vec(input_data=Data,save_loc=model_save_location,vec_dims=vector_dims,alpha_=alpha,size_of_window=size_window,no_workers=num_workers,max_epochs=max_num_epochs,min_alpha_=min_alpha,minimum_count=mini_count,dms=dm)\n",
        "  elif name == 'lsi':\n",
        "    lsi(input_data=Data,save_loc=model_save_location,total_topics=topics)\n",
        "  elif name == 'tfidf':\n",
        "    tfidf(input_data=Data,save_loc=model_save_location)\n",
        "\n",
        "def word2vec(input_data,save_loc,vec_dims,SG,size_of_window,minimum_count,no_workers):\n",
        "  model = Word2Vec(input_data,min_count=minimum_count,size= vec_dims,workers=no_workers, window =size_of_window, sg = SG)\n",
        "  model.save(save_loc)\n",
        "\n",
        "def doc2vec(input_data,save_loc,vec_dims,alpha_,size_of_window,min_alpha_,minimum_count,dms,no_workers,max_epochs):\n",
        "  model = Doc2Vec(size=vec_dims,\n",
        "                alpha=alpha_, \n",
        "                min_alpha=min_alpha_,\n",
        "                window = size_of_window,\n",
        "                min_count=minimum_count,\n",
        "                dm =dms)\n",
        "  model.build_vocab(input_data)\n",
        "\n",
        "  for epoch in range(max_epochs):\n",
        "    #print('iteration {0}'.format(epoch))\n",
        "    model.train(input_data, total_examples=model.corpus_count, epochs=model.iter)\n",
        "    # decrease the learning rate\n",
        "    model.alpha -= 0.0002\n",
        "    # fix the learning rate, no decay\n",
        "    model.min_alpha = model.alpha\n",
        "  model.save(save_loc)\n",
        "\n",
        "def lsi(input_data,save_loc,total_topics):\n",
        "  model = models.LsiModel(corpus=input_data, num_topics=total_topics)\n",
        "  index = similarities.MatrixSimilarity(model[input_data])\n",
        "  lsi_data = model[input_data]\n",
        "  lsi_topics = model.print_topics()\n",
        "  #for topic in lsi_topics:\n",
        "   # print(topic)\n",
        "  model.save(save_loc)\n",
        "\n",
        "def tfidf(input_data,save_loc):\n",
        "  model = models.TfidfModel(corpus=input_data)\n",
        "  tfidf_data = model[input_data]\n",
        "\n",
        "  tfidf_token= np.zeros((len(tfidf_data), 350), dtype=np.float64)\n",
        "  tfidf_vals= np.zeros((len(tfidf_data), 350), dtype=np.float64)\n",
        " \n",
        "  for i in range(len(input_data)):\n",
        "    for k in range(len(list(tfidf_data)[i])):\n",
        "      tfidf_token[i][k]=(list(tfidf_data))[i][k][0]\n",
        "      tfidf_vals[i][k]=(list(tfidf_data))[i][k][1]\n",
        "  tfidf_list=list(tfidf_data)\n",
        "  #print(list(tfidf_data))\n",
        "  model.save('tfidf_model')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXdIQWmIGFOG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "outputId": "edf29a90-38ec-4084-9799-c0bb8bd5f22c"
      },
      "source": [
        "corpus_data, num_users = data_module(name='word2vec',max_seq_length=100,location='/content/drive/My Drive/Movielensdata/ml25m/ratings.csv')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iWoBWvRZwSD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "d39ffa63-af7b-4886-c58c-4959b835c8de"
      },
      "source": [
        "embedding_model(name='word2vec',Data=corpus_data,model_save_location='/content/drive/My Drive/Movielensdata/ml25m/word2vec/w2v',vector_dims=50,Sg=1,size_window=3,topics=10,mini_count=1,num_workers=3,max_num_epochs = 100,alpha = 0.025,min_alpha=0.00025,dm=1)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IY5ihcX7bma_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "62c4659f-a7de-4433-c004-1e35f211cf39"
      },
      "source": [
        "print(corpus_data)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['5952', '2012', '2011', '1653', '1250', '6539', '6377', '3448', '1088', '899', '4308', '2161', '6711', '3949', '8360', '5878', '306', '1175', '307', '1237', '7327', '8154', '7234', '2843', '4144', '7365', '2068', '4422', '4973', '6016', '8873', '2692', '27721', '7323', '6954', '8014', '7939', '6370', '8973', '4703', '31956', '5147', '8786', '1260', '2351', '7940', '7209', '8685', '7820', '7937', '7938', '8405', '4325', '2632', '1217', '8729', '5912', '5767', '665', '2573', '27266', '8327', '32591', '5269', '3569', '27193', '5684', '7318', '296', '7361', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0'], ['4306', '8368', '8360', '3793', '4995', '4963', '8636', '2355', '8665', '2571', '589', '33493', '7153', '1873', '1246', '1584', '3994', '2139', '31923', '4720', '2294', '2745', '2138', '858', '33660', '2268', '2501', '6539', '1270', '6565', '4535', '1198', '1302', '349', '3098', '1907', '1580', '364', '480', '3360', '524', '5103', '62', '7075', '1287', '260', '318', '588', '1693', '1196', '4993', '2918', '3105', '5955', '1275', '1393', '6879', '5010', '6311', '8970', '1356', '3175', '4016', '1672', '1257', '2115', '36527', '1674', '1376', '7624', '2359', '8958', '35836', '2761', '953', '1923', '6947', '3148', '2083', '8010', '7162', '914', '4023', '33166', '3396', '1527', '2324', '1271', '30848', '30749', '534', '4103', '4874', '261', '1465', '2987', '1293', '7090', '1136', '2150'], ['176371', '3968', '8983', '102407', '6548', '4448', '103141', '139644', '187593', '135143', '90866', '86911', '27660', '171763', '8861', '143385', '112138', '5313', '4958', '136020', '6879', '59501', '122906', '111659', '88405', '158238', '179819', '99112', '166635', '135536', '97306', '65514', '135569', '187541', '136864', '57528', '1960', '96079', '94677', '97923', '46335', '159093', '86644', '112171', '59900', '67923', '71464', '71156', '5900', '62999', '68793', '130634', '81537', '53129', '89753', '68952', '140956', '97225', '102716', '104241', '70336', '111113', '129354', '45666', '82202', '54648', '102686', '39435', '96861', '61248', '31221', '158966', '85020', '83349', '79224', '64030', '105213', '56156', '107348', '176101', '103810', '49822', '91976', '73323', '149406', '122896', '103384', '168612', '91974', '55805', '111443', '109578', '87876', '94018', '79091', '88125', '106489', '81834', '4262', '1732'], ['8665', '8961', '4993', '1580', '5444', '7153', '6539', '33794', '6874', '1210', '1610', '7438', '780', '34405', '1214', '2028', '2985', '1291', '260', '5618', '1907', '38038', '2571', '44665', '8874', '1198', '1196', '296', '44191', '1527', '7143', '8641', '589', '1220', '5418', '4308', '8914', '4886', '6377', '4816', '34048', '5903', '7373', '6947', '5219', '3827', '5110', '6156', '45431', '7324', '44193', '4974', '6550', '7451', '1036', '3751', '6863', '5299', '3624', '4709', '924', '1265', '1201', '3681', '7569', '2993', '1732', '2019', '2918', '3175', '1278', '1080', '3114', '3671', '2791', '1288', '1270', '1', '541', '1200', '2716', '3039', '2115', '2951', '3033', '4963', '1197', '5952', '3793', '1136', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCNUWaQIbmfG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e5de379e-c27e-423a-df4b-fbe49ff3f12a"
      },
      "source": [
        "print(num_users)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WqzalHzc_lp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 7,
      "outputs": []
    }
  ]
}