{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Code_model.ipynb",
      "provenance": [],
      "mount_file_id": "1wgpB7rU7rrk4BnoapdXtGyFmepqlRw6K",
      "authorship_tag": "ABX9TyOuV+buuxkMkr1tO7hiFaLJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dharani1999/Word-embedding-techniques/blob/master/Code_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTPPDaIkL-DE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import gensim\n",
        "from gensim import corpora, models, similarities\n",
        "from gensim.models import Word2Vec, TfidfModel, LsiModel\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EslXGch8IZgd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_module(name,location,output_location,max_seq_length=100):\n",
        "  dataset1 = pd.read_csv(location)\n",
        "  dataset = dataset1.iloc[0:1000,:]\n",
        "\n",
        "  if name == 'word2vec':\n",
        "    data_corpus, users_total = data_word2vec(dataset,max_seq_length)\n",
        "  elif name == 'doc2vec':\n",
        "    data_corpus, users_total = data_doc2vec(dataset,max_seq_length)\n",
        "  elif name == 'lsi':\n",
        "    data_corpus, users_total = data_lsi(dataset,max_seq_length)\n",
        "  elif name == 'tfidf':\n",
        "    data_corpus, users_total = data_tfidf(dataset,max_seq_length)\n",
        "  #data_corpus.save('/content/drive/My Drive/Movielensdata/ml25m/data_corpus')\n",
        "  #print(data_corpus)\n",
        "  save_object(obj=data_corpus, filename=output_location)\n",
        "  #return data_corpus, users_total\n",
        "\n",
        "def data_word2vec(dataset,max_seq_length):\n",
        "  dataset.sort_values(by=['userId','timestamp'],inplace=True)\n",
        "  user_total = len(dataset['userId'].unique())\n",
        "  \n",
        "  #Selecting the most recent movies rated by each user and padding if necessary\n",
        "  movie_list = []\n",
        "  for i in range(user_total):\n",
        "    list1 = []\n",
        "    list1 = dataset.loc[dataset['userId'] ==(i+1),['movieId']]['movieId'].tolist()\n",
        "    if len(list1)>max_seq_length:\n",
        "      list1 = list1[(len(list1)-max_seq_length):]\n",
        "    elif len(list1)<max_seq_length:\n",
        "      list1 = list1+[0 for j in range((max_seq_length-len(list1)))]\n",
        "      #for j in range((max_seq_length-len(list1))):\n",
        "       # list1.append(0)\n",
        "    movie_list.append(list1)\n",
        "  \n",
        "  #Selecting the most recent ratings rated by each user and padding if necessary\n",
        "  rating_list =[]\n",
        "  for i in range(user_total):\n",
        "    list2 = []\n",
        "    list2 = dataset.loc[dataset['userId'] ==(i+1),['rating']]['rating'].tolist()\n",
        "    if len(list2)>max_seq_length:\n",
        "      list2 = list2[(len(list2)-max_seq_length):]\n",
        "    elif len(list2)<max_seq_length:\n",
        "      list2 = list2+[0 for j in range((max_seq_length-len(list2)))]\n",
        "      #for j in range((max_seq_length-len(list2))):\n",
        "       # list2.append(0)\n",
        "    rating_list.append(list2)\n",
        "  \n",
        "  #Creating user_id level transpose matrices\n",
        "  movies_transpose = pd.DataFrame(data=movie_list,index=[i+1 for i in range(user_total)])\n",
        "  movies_transpose.index.names = ['userId']\n",
        "  #print(movies_transpose)\n",
        "\n",
        "  ratings_transpose = pd.DataFrame(data=rating_list,index=[i+1 for i in range(user_total)])\n",
        "  ratings_transpose.index.names = ['userId']\n",
        "  #print(ratings_transpose)\n",
        "\n",
        "  # Select features from original dataset to form a new dataframe \n",
        "  df1 = movies_transpose.iloc[:]# For each row, combine all the columns into one column\n",
        "  df2 = df1.apply(lambda x: ','.join(x.astype(str)), axis=1)# Store them in a pandas dataframe\n",
        "  df_clean = pd.DataFrame({'clean': df2})# Create the list of list format of the custom corpus for gensim modeling \n",
        "  sent = [row.split(',') for row in df_clean['clean']]\n",
        "\n",
        "  return sent, user_total\n",
        "\n",
        "def data_doc2vec(dataset,max_seq_length):\n",
        "  Sent, user_total = data_word2vec(dataset,max_seq_length)\n",
        "  tagged_data = []\n",
        "  tags = []\n",
        "  \n",
        "  for i in range(user_total):\n",
        "    tagged_data = tagged_data + [TaggedDocument(words=Sent[i], tags=[str(i)])]\n",
        "\n",
        "  return tagged_data, user_total\n",
        "\n",
        "def data_lsi(dataset,max_seq_length):\n",
        "  Sent, user_total = data_word2vec(dataset,max_seq_length)\n",
        "  dictionary = corpora.Dictionary(sent)\n",
        "  #print(dictionary.token2id)\n",
        "  corp = [dictionary.doc2bow(text) for text in Sent]\n",
        "  corpus = np.array([[(id, freq) for id, freq in cp] for cp in corp])\n",
        "  #corpus = gensim.matutils.Dense2Corpus(np.array(Sent),documents_columns=False)\n",
        "\n",
        "  return corpus, user_total\n",
        "\n",
        "def data_tfidf(dataset,max_seq_length):\n",
        "  Sent, user_total = data_word2vec(dataset,max_seq_length)\n",
        "  dictionary = corpora.Dictionary(sent)\n",
        "  #print(dictionary.token2id)\n",
        "  corp = [dictionary.doc2bow(text) for text in Sent]\n",
        "  corpus = np.array([[(id, freq) for id, freq in cp] for cp in corp])\n",
        "  #corpus = gensim.matutils.Dense2Corpus(np.array(Sent),documents_columns=False)\n",
        "  \n",
        "  return corpus, user_total"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nrx8P2P32kT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_object(obj, filename):\n",
        "    with open(filename, 'wb') as output:  # Overwrites any existing file.\n",
        "        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "def load_object(filename):\n",
        "    with open(filename, 'rb') as input:\n",
        "        pickle_object = pickle.load(input)\n",
        "    return  pickle_object"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kJytoDNK3E0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def embedding_model(name,Data_location,model_save_location,vector_dims=50,Sg=1,size_window=3,topics=10,mini_count=1,num_workers=3,max_num_epochs = 100,alpha = 0.025,min_alpha=0.00025,dm=1):\n",
        "  Data = load_object(Data_location)\n",
        "  #print(Data)\n",
        "  if name == 'word2vec':\n",
        "    word2vec(input_data=Data,save_loc=model_save_location,vec_dims=vector_dims,SG=Sg,size_of_window=size_window,minimum_count=mini_count,no_workers=num_workers)\n",
        "    #voc = model1.wv\n",
        "    #words = list(model1.wv.vocab)\n",
        "    #vectors = model1[model1.wv.vocab]\n",
        "  elif name == 'doc2vec':\n",
        "    doc2vec(input_data=Data,save_loc=model_save_location,vec_dims=vector_dims,alpha_=alpha,size_of_window=size_window,no_workers=num_workers,max_epochs=max_num_epochs,min_alpha_=min_alpha,minimum_count=mini_count,dms=dm)\n",
        "  elif name == 'lsi':\n",
        "    lsi(input_data=Data,save_loc=model_save_location,total_topics=topics)\n",
        "  elif name == 'tfidf':\n",
        "    tfidf(input_data=Data,save_loc=model_save_location)\n",
        "\n",
        "def word2vec(input_data,save_loc,vec_dims,SG,size_of_window,minimum_count,no_workers):\n",
        "  model = Word2Vec(input_data,min_count=minimum_count,size= vec_dims,workers=no_workers, window =size_of_window, sg = SG)\n",
        "  model.save(save_loc)\n",
        "\n",
        "def doc2vec(input_data,save_loc,vec_dims,alpha_,size_of_window,min_alpha_,minimum_count,dms,no_workers,max_epochs):\n",
        "  model = Doc2Vec(size=vec_dims,\n",
        "                alpha=alpha_, \n",
        "                min_alpha=min_alpha_,\n",
        "                window = size_of_window,\n",
        "                min_count=minimum_count,\n",
        "                dm =dms)\n",
        "  model.build_vocab(input_data)\n",
        "\n",
        "  for epoch in range(max_epochs):\n",
        "    #print('iteration {0}'.format(epoch))\n",
        "    model.train(input_data, total_examples=model.corpus_count, epochs=model.iter)\n",
        "    # decrease the learning rate\n",
        "    model.alpha -= 0.0002\n",
        "    # fix the learning rate, no decay\n",
        "    model.min_alpha = model.alpha\n",
        "  model.save(save_loc)\n",
        "\n",
        "def lsi(input_data,save_loc,total_topics):\n",
        "  model = models.LsiModel(corpus=input_data, num_topics=total_topics)\n",
        "  index = similarities.MatrixSimilarity(model[input_data])\n",
        "  lsi_data = model[input_data]\n",
        "  lsi_topics = model.print_topics()\n",
        "  #for topic in lsi_topics:\n",
        "    #print(topic)\n",
        "  model.save(save_loc)\n",
        "\n",
        "def tfidf(input_data,save_loc):\n",
        "  model = models.TfidfModel(corpus=input_data)\n",
        "  tfidf_data = model[input_data]\n",
        "\n",
        "  tfidf_token= np.zeros((len(tfidf_data), 350), dtype=np.float64)\n",
        "  tfidf_vals= np.zeros((len(tfidf_data), 350), dtype=np.float64)\n",
        " \n",
        "  for i in range(len(input_data)):\n",
        "    for k in range(len(list(tfidf_data)[i])):\n",
        "      tfidf_token[i][k]=(list(tfidf_data))[i][k][0]\n",
        "      tfidf_vals[i][k]=(list(tfidf_data))[i][k][1]\n",
        "  tfidf_list=list(tfidf_data)\n",
        "  #print(list(tfidf_data))\n",
        "  model.save('tfidf_model')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXlM6wLvQVIW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scoring_module(name,model_loc,data):\n",
        "  if name == 'word2vec':\n",
        "    scores_list = score_word2vec(model_location=model_loc,input_data=data)\n",
        "  if name == 'doc2vec':\n",
        "    scores_list = score_doc2vec(model_location=model_loc,input_data=data)\n",
        "  if name == 'lsi':\n",
        "    scores_list = score_lsi(model_location=model_loc,input_data=data)\n",
        "  if name == 'tfidf':\n",
        "    scores_list = score_tfidf(model_location=model_loc,input_data=data)\n",
        "  \n",
        "  return scores_list"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERfUh5LeRIpz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def score_word2vec(model_location,input_data):\n",
        "  model = Word2Vec.load(model_location)\n",
        "  scored_data = model[input_data] \n",
        "  return scored_data"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeJqp5MlRI8n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def score_doc2vec(model_location,input_data):\n",
        "  model = Doc2Vec.load(model_location)\n",
        "  scored_data = model[input_data]\n",
        "  return scored_data"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjDWm8acRJD_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def score_lsi(model_location,input_data):\n",
        "  model = LsiModel.load(model_location)\n",
        "  scored_data = model[input_data]\n",
        "  return scored_data"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-9qzlpJRJb0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def score_tfidf(model_location,input_data):\n",
        "  model = TfidfModel.load(model_location)\n",
        "  scored_data = model[input_data]\n",
        "  return scored_data"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXdIQWmIGFOG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "08ba89e3-70d2-4e51-9d03-6afee8c72eda"
      },
      "source": [
        "data_module(name='word2vec',max_seq_length=100,location='/content/drive/My Drive/Movielensdata/ml25m/ratings.csv',output_location='/content/drive/My Drive/Movielensdata/ml25m/data_corpus/data')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iWoBWvRZwSD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "1b1cd6fd-4241-48f7-cf15-78293d0106e1"
      },
      "source": [
        "embedding_model(name='word2vec',Data_location='/content/drive/My Drive/Movielensdata/ml25m/data_corpus/data',model_save_location='/content/drive/My Drive/Movielensdata/ml25m/word2vec/w2v',vector_dims=50,Sg=1,size_window=3,topics=10,mini_count=1,num_workers=3,max_num_epochs = 100,alpha = 0.025,min_alpha=0.00025,dm=1)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WqzalHzc_lp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scoring_input = ['899', '2161', '3949', '5878', '1175', '1237', '8154', '2843', '7365', '4422', '6016', '1080', '3114', '3671', '2791', '1288', '1', '541', '2692', '7323', '8014', '6370', '4703', '5147']"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ry_oK4hcUcJb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "outputId": "9e3c50c9-cc86-408e-cecf-c919234aa0c7"
      },
      "source": [
        "scored_list = scoring_module(name='word2vec',model_loc='/content/drive/My Drive/Movielensdata/ml25m/word2vec/w2v',data=scoring_input)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEHVPbZmaO3M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "outputId": "f73e2a17-e621-4468-ffc1-c275bf1020ee"
      },
      "source": [
        "print(scored_list[0])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.00459596 -0.00241911  0.00612697 -0.01001128  0.00030483 -0.0042486\n",
            "  0.00780082  0.00636277  0.00383855 -0.00856883 -0.00715745 -0.00479852\n",
            " -0.0006926   0.00282047  0.00216675  0.00863381  0.00512881 -0.00274457\n",
            "  0.00404377 -0.005483    0.00731773  0.00408469  0.00272944  0.0080032\n",
            " -0.00147792 -0.00099985  0.00182403  0.00825111 -0.00581281  0.00939213\n",
            "  0.0007721  -0.00075137 -0.00421297  0.00307526 -0.00747749 -0.0002278\n",
            " -0.00612344  0.00993508 -0.00213574  0.00380339  0.0031376   0.00041305\n",
            " -0.00645378  0.00744731 -0.00217777  0.00680043  0.0016865  -0.00305453\n",
            " -0.00209291  0.00616063]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Io4aqWLneCm1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "outputId": "e0e0a133-be47-44e0-f7aa-ced9a3ae5bf1"
      },
      "source": [
        "scored_list"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.00459596, -0.00241911,  0.00612697, ..., -0.00305453,\n",
              "        -0.00209291,  0.00616063],\n",
              "       [ 0.0084547 ,  0.00522159, -0.00043755, ..., -0.00181077,\n",
              "         0.00937611,  0.00562771],\n",
              "       [-0.00930926, -0.00691647, -0.00987944, ..., -0.00550414,\n",
              "        -0.00242318, -0.00017445],\n",
              "       ...,\n",
              "       [-0.00838138, -0.00657606, -0.00526562, ..., -0.00740144,\n",
              "         0.00497558,  0.00963203],\n",
              "       [-0.00896329,  0.00793444,  0.00134797, ...,  0.00510431,\n",
              "        -0.00557781, -0.00035319],\n",
              "       [-0.00168067,  0.00494596,  0.0040232 , ..., -0.00881966,\n",
              "        -0.00635621, -0.00876874]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    }
  ]
}