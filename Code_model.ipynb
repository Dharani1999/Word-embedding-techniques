{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Code_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1wgpB7rU7rrk4BnoapdXtGyFmepqlRw6K",
      "authorship_tag": "ABX9TyNb9LY/chWXK5oOrkgW4gYd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dharani1999/Word-embedding-techniques/blob/master/Code_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTPPDaIkL-DE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "61f92a4f-b256-42e9-b42d-ed1c266c13b6"
      },
      "source": [
        "import pickle\n",
        "import gensim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import nltk\n",
        "import time\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from gensim.parsing.preprocessing import stem_text\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "from gensim import corpora, models, similarities\n",
        "from gensim.models import Word2Vec, TfidfModel, LsiModel\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from gensim.models.fasttext import FastText\n",
        "\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorboard as tb\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Conv2D,MaxPooling2D,Activation,Dense,Flatten,Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.python.framework import ops\n",
        "#tf.gfile = tf.io.gfile\n",
        "#tf.io.gfile = tb.compat.tensorflow_stub.io.gfile\n",
        "\n",
        "import random\n",
        "import pathlib\n",
        "import matplotlib.pyplot as plt\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4XGZScCdZgS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "outputId": "655bf2ed-996d-4c47-a0ee-06ae2ab81892"
      },
      "source": [
        "#!pip install git+https://github.com/huggingface/transformers\n",
        "!pip install glove_python\n",
        "from glove import Corpus, Glove# creating a corpus object"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting glove_python\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3e/79/7e7e548dd9dcb741935d031117f4bed133276c2a047aadad42f1552d1771/glove_python-0.1.0.tar.gz (263kB)\n",
            "\r\u001b[K     |█▎                              | 10kB 8.4MB/s eta 0:00:01\r\u001b[K     |██▌                             | 20kB 1.5MB/s eta 0:00:01\r\u001b[K     |███▊                            | 30kB 1.9MB/s eta 0:00:01\r\u001b[K     |█████                           | 40kB 1.5MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 51kB 1.8MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 61kB 2.1MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 71kB 2.3MB/s eta 0:00:01\r\u001b[K     |██████████                      | 81kB 2.5MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 92kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 102kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 112kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 122kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 133kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 143kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 153kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 163kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 174kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 184kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 194kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 204kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 215kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 225kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 235kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 245kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 256kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 266kB 2.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from glove_python) (1.18.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from glove_python) (1.4.1)\n",
            "Building wheels for collected packages: glove-python\n",
            "  Building wheel for glove-python (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for glove-python: filename=glove_python-0.1.0-cp36-cp36m-linux_x86_64.whl size=700293 sha256=0a71a8958a6feae378382d5c206172ee0a5e40a9d5013cba082efd7b8e5d131f\n",
            "  Stored in directory: /root/.cache/pip/wheels/88/4b/6d/10c0d2ad32c9d9d68beec9694a6f0b6e83ab1662a90a089a4b\n",
            "Successfully built glove-python\n",
            "Installing collected packages: glove-python\n",
            "Successfully installed glove-python-0.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nrx8P2P32kT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_object(obj, filename):\n",
        "    with open(filename, 'wb') as output:  # Overwrites any existing file.\n",
        "        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "def load_object(filename):\n",
        "    with open(filename, 'rb') as input:\n",
        "        pickle_object = pickle.load(input)\n",
        "    return  pickle_object"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtHGs7qlVUsn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        },
        "outputId": "66763ea8-1bb5-4a0f-873d-4e7d85a9ba3e"
      },
      "source": [
        "!mkdir imdb\n",
        "\n",
        "def pre_process(data):\n",
        "  _sent=[]\n",
        "  for i in data:\n",
        "    sent = word_tokenize(i)\n",
        "    sent = [w.lower() for w in sent]\n",
        "    #sent = [word for word in sent if word.isalpha()]\n",
        "    sent = [c for c in sent if not c.isdigit()]\n",
        "    sent = [stem_text(w) for w in sent]\n",
        "    #sent = [wordnet_lemmatizer.lemmatize(w) for w in sent]\n",
        "    #sent = [word for word in sent if word not in stoplist]\n",
        "    _sent.append(sent)\n",
        "  return _sent\n",
        "\n",
        "from keras.datasets import imdb\n",
        "data = imdb.load_data(path='/content/imdb/data',\n",
        "    num_words=None,\n",
        "    skip_top=0,\n",
        "    maxlen=None,\n",
        "    seed=113,\n",
        "    start_char=1,\n",
        "    oov_char=2,\n",
        "    index_from=3)\n",
        "save_object(data, '/content/imdb/data')\n",
        "word_to_idx_dict = imdb.get_word_index(path=\"/content/imdb/imdb_word_index.json\")\n",
        "n = len(word_to_idx_dict)\n",
        "print(\"Number distinct words in reviews = %d \\n\" % n)\n",
        "\n",
        "words_freq_list = []\n",
        "for (k,v) in word_to_idx_dict.items():\n",
        "  words_freq_list.append((k,v))\n",
        "\n",
        "sorted_list = sorted(words_freq_list, key=lambda x: x[1])\n",
        "\n",
        "print(\"Ten most common words: \\n\")\n",
        "print(sorted_list[0:10])\n",
        "\n",
        "print(\"\\nLast five least common words: \\n\")\n",
        "print(sorted_list[-5:])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 2s 0us/step\n",
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 1s 1us/step\n",
            "Number distinct words in reviews = 88584 \n",
            "\n",
            "Ten most common words: \n",
            "\n",
            "[('the', 1), ('and', 2), ('a', 3), ('of', 4), ('to', 5), ('is', 6), ('br', 7), ('in', 8), ('it', 9), ('i', 10)]\n",
            "\n",
            "Last five least common words: \n",
            "\n",
            "[(\"pipe's\", 88580), ('copywrite', 88581), ('artbox', 88582), (\"voorhees'\", 88583), (\"'l'\", 88584)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZszdGlsnK4V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#d = load_object('/content/imdb/data')\n",
        "#d"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EslXGch8IZgd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_module(reviews, name, location, output_location, dict_location, max_seq_length=100):\n",
        "  if reviews==False:\n",
        "    dataset1 = pd.read_csv(location)\n",
        "    dataset = dataset1.iloc[0:1000,:]\n",
        "  else:\n",
        "    dataset = load_object(location)\n",
        "  if name == 'word2vec':\n",
        "    data_corpus, users_total = data_word2vec(reviews,dataset,max_seq_length)\n",
        "  elif name == 'doc2vec':\n",
        "    data_corpus, users_total = data_doc2vec(reviews,dataset,max_seq_length)\n",
        "  elif name == 'lsi':\n",
        "    data_corpus, users_total = data_lsi(reviews,dataset,max_seq_length,dict_loc=dict_location)\n",
        "  elif name == 'tfidf':\n",
        "    data_corpus, users_total = data_lsi(reviews,dataset,max_seq_length,dict_loc=dict_location)\n",
        "  elif name == 'glove':\n",
        "    data_corpus, users_total = data_word2vec(reviews,dataset,max_seq_length)\n",
        "  elif name == 'hashing':\n",
        "    data_corpus, users_total = data_hashing(reviews,dataset,max_seq_length)\n",
        "  elif name == 'cooccur':\n",
        "    data_corpus, users_total = data_hashing(reviews,dataset,max_seq_length)\n",
        "  #elif name == 'fasttext':\n",
        "   # data_corpus, users_total = data_word2vec(dataset,max_seq_length)\n",
        "  #data_corpus.save('/content/drive/My Drive/Movielensdata/ml25m/data_corpus')\n",
        "  #print(data_corpus)\n",
        "  save_object(obj=data_corpus, filename=output_location)\n",
        "  #return data_corpus, users_total\n",
        "\n",
        "def data_word2vec(review,dataset,max_seq_length):\n",
        "  if review==False:\n",
        "    dataset.sort_values(by=['userId','timestamp'],inplace=True)\n",
        "    user_total = len(dataset['userId'].unique())\n",
        "    \n",
        "    #Selecting the most recent movies rated by each user and padding if necessary\n",
        "    movie_list = []\n",
        "    for i in range(user_total):\n",
        "      list1 = []\n",
        "      list1 = dataset.loc[dataset['userId'] ==(i+1),['movieId']]['movieId'].tolist()\n",
        "      if len(list1)>max_seq_length:\n",
        "        list1 = list1[(len(list1)-max_seq_length):]\n",
        "      elif len(list1)<max_seq_length:\n",
        "        list1 = list1+[0 for j in range((max_seq_length-len(list1)))]\n",
        "        #for j in range((max_seq_length-len(list1))):\n",
        "        # list1.append(0)\n",
        "      movie_list.append(list1)\n",
        "\n",
        "    #Selecting the most recent ratings rated by each user and padding if necessary\n",
        "    rating_list =[]\n",
        "    for i in range(user_total):\n",
        "      list2 = []\n",
        "      list2 = dataset.loc[dataset['userId'] ==(i+1),['rating']]['rating'].tolist()\n",
        "      if len(list2)>max_seq_length:\n",
        "        list2 = list2[(len(list2)-max_seq_length):]\n",
        "      elif len(list2)<max_seq_length:\n",
        "        list2 = list2+[0 for j in range((max_seq_length-len(list2)))]\n",
        "        #for j in range((max_seq_length-len(list2))):\n",
        "        # list2.append(0)\n",
        "      rating_list.append(list2)\n",
        "\n",
        "    #Creating user_id level transpose matrices\n",
        "    movies_transpose = pd.DataFrame(data=movie_list,index=[i+1 for i in range(user_total)])\n",
        "    movies_transpose.index.names = ['userId']\n",
        "    #print(movies_transpose)\n",
        "\n",
        "    ratings_transpose = pd.DataFrame(data=rating_list,index=[i+1 for i in range(user_total)])\n",
        "    ratings_transpose.index.names = ['userId']\n",
        "    #print(ratings_transpose)\n",
        "\n",
        "    # Select features from original dataset to form a new dataframe \n",
        "    df1 = movies_transpose.iloc[:]# For each row, combine all the columns into one column\n",
        "    df2 = df1.apply(lambda x: ','.join(x.astype(str)), axis=1)# Store them in a pandas dataframe\n",
        "    df_clean = pd.DataFrame({'clean': df2})# Create the list of list format of the custom corpus for gensim modeling \n",
        "    sent = [row.split(',') for row in df_clean['clean']]\n",
        "\n",
        "    return sent, user_total\n",
        "  else:\n",
        "    X_train = dataset[0][0]\n",
        "    Y_train = dataset[0][1]\n",
        "    X_test, X_val, Y_test, Y_val = train_test_split(data[1][0], data[1][1], test_size=0.4, shuffle=False)\n",
        "    \n",
        "    reverse_word_index = dict([(value, key) for (key, value) in word_to_idx_dict.items()])\n",
        "    train_reviews=[]\n",
        "    for j in X_train:\n",
        "      train_reviews.append(' '.join([reverse_word_index.get(i - 3, '') for i in j]))\n",
        "\n",
        "    test_reviews=[]\n",
        "    for j in X_test:\n",
        "      test_reviews.append(' '.join([reverse_word_index.get(i - 3, '') for i in j]))\n",
        "\n",
        "    val_reviews=[]\n",
        "    for j in X_val:\n",
        "      val_reviews.append(' '.join([reverse_word_index.get(i - 3, '') for i in j]))\n",
        "\n",
        "    train_sent=pre_process(train_reviews)\n",
        "    save_object(train_sent,'/content/imdb/train_sent')\n",
        "    test_sent=pre_process(test_reviews)\n",
        "    save_object(test_sent,'/content/imdb/test_sent')\n",
        "    val_sent=pre_process(val_reviews)\n",
        "    save_object(val_sent,'/content/imdb/val_sent')\n",
        "    save_object(Y_train, '/content/imdb/Y_train')\n",
        "    save_object(Y_test, '/content/imdb/Y_test')\n",
        "    save_object(Y_val, '/content/imdb/Y_val')\n",
        "\n",
        "    return train_sent, len(train_sent)\n",
        "\n",
        "def data_doc2vec(review,dataset,max_seq_length):\n",
        "  Sent, user_total = data_word2vec(review,dataset,max_seq_length)\n",
        "  tagged_data = []\n",
        "  tags = []\n",
        "  \n",
        "  for i in range(user_total):\n",
        "    tagged_data = tagged_data + [TaggedDocument(words=Sent[i], tags=[str(i)])]\n",
        "\n",
        "  return tagged_data, user_total\n",
        "\n",
        "def data_lsi(review,dataset,max_seq_length,dict_loc):\n",
        "  Sent, user_total = data_word2vec(review,dataset,max_seq_length)\n",
        "  dictionary = corpora.Dictionary(Sent)\n",
        "  #print(dictionary.token2id)\n",
        "  corpus = [dictionary.doc2bow(text) for text in Sent]\n",
        "  dictionary.save(dict_loc)\n",
        "  #corpus = np.array([[(id, freq) for id, freq in cp] for cp in corp])\n",
        "  #corpus = gensim.matutils.Dense2Corpus(np.array(Sent),documents_columns=False)\n",
        "\n",
        "  return corpus, user_total\n",
        "\n",
        "def data_hashing(review,dataset,max_seq_length):\n",
        "  Sent, user_total = data_word2vec(review,dataset,max_seq_length)\n",
        "  corpus = [str(str(doc)[1:-1]) for doc in Sent]\n",
        "  return corpus, user_total"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kJytoDNK3E0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def embedding_model(name,Data_location,model_save_location,matrix_location, maxi_features, vector_dims=10,Sg=1,size_window=3,topics=10,mini_count=1,num_workers=3,max_num_epochs = 100,alpha = 0.025,min_alpha=0.00025,dm=1):\n",
        "  Data = load_object(Data_location)\n",
        "  #print(Data)\n",
        "  if name == 'word2vec':\n",
        "    word2vec(input_data=Data, save_loc=model_save_location, vec_dims=vector_dims, SG=Sg, size_of_window=size_window, minimum_count=mini_count, no_workers=num_workers)\n",
        "    #voc = model1.wv\n",
        "    #words = list(model1.wv.vocab)\n",
        "    #vectors = model1[model1.wv.vocab]\n",
        "  elif name == 'doc2vec':\n",
        "    doc2vec(input_data=Data, save_loc=model_save_location, vec_dims=vector_dims, alpha_=alpha, size_of_window=size_window, no_workers=num_workers, max_epochs=max_num_epochs,min_alpha_=min_alpha, minimum_count=mini_count, dms=dm)\n",
        "  elif name == 'lsi':\n",
        "    lsi(input_data=Data, save_loc=model_save_location, total_topics=topics)\n",
        "  elif name == 'tfidf':\n",
        "    tfidf(input_data=Data,save_loc=model_save_location)\n",
        "  elif name == 'glove':\n",
        "    glove_model(input_data=Data, vec_dims=vector_dims, size_of_window=size_window, save_loc=model_save_location, num_epochs=max_num_epochs, alpha_=0.05, num_threads=4)\n",
        "  elif name == 'hashing':\n",
        "    hashing(input_data=Data, vec_dims=vector_dims, save_loc=model_save_location)\n",
        "  elif name == 'cooccur':\n",
        "    co_occur(input_data=Data, maximum_features=maxi_features, save_loc=model_save_location, matrix_loc = matrix_location)\n",
        "  #elif name == 'fasttext':\n",
        "   # fast_text(input_data=Data, save_loc=model_save_location, vec_dims=vector_dims, SG=Sg, size_of_window=size_window, minimum_count=mini_count, no_workers=num_workers, alpha_=0.025)\n",
        "\n",
        "def word2vec(input_data,save_loc,vec_dims,SG,size_of_window,minimum_count,no_workers):\n",
        "  model = Word2Vec(input_data,min_count=minimum_count,size= vec_dims,workers=no_workers, window =size_of_window, sg = SG)\n",
        "  model.save(save_loc)\n",
        "\n",
        "def doc2vec(input_data,save_loc,vec_dims,alpha_,size_of_window,min_alpha_,minimum_count,dms,no_workers,max_epochs):\n",
        "  model = Doc2Vec(size=vec_dims,\n",
        "                alpha=alpha_, \n",
        "                min_alpha=min_alpha_,\n",
        "                window = size_of_window,\n",
        "                min_count=minimum_count,\n",
        "                dm =dms)\n",
        "  model.build_vocab(input_data)\n",
        "\n",
        "  for epoch in range(max_epochs):\n",
        "    #print('iteration {0}'.format(epoch))\n",
        "    model.train(input_data, total_examples=model.corpus_count, epochs=model.iter)\n",
        "    # decrease the learning rate\n",
        "    model.alpha -= 0.0002\n",
        "    # fix the learning rate, no decay\n",
        "    model.min_alpha = model.alpha\n",
        "  model.save(save_loc)\n",
        "\n",
        "def lsi(input_data,save_loc,total_topics):\n",
        "  model = models.LsiModel(corpus=input_data, num_topics=total_topics)\n",
        "  index = similarities.MatrixSimilarity(model[input_data])\n",
        "  lsi_data = model[input_data]\n",
        "  lsi_topics = model.print_topics()\n",
        "  #for topic in lsi_topics:\n",
        "    #print(topic)\n",
        "  model.save(save_loc)\n",
        "\n",
        "def tfidf(input_data,save_loc):\n",
        "  model = models.TfidfModel(corpus=input_data, normalize=True)\n",
        "  tfidf_data = model[input_data]\n",
        "  model.save(save_loc)\n",
        "\n",
        "def glove_model(input_data,vec_dims,size_of_window,save_loc,alpha_=0.05,num_epochs=30, num_threads=4):\n",
        "  #importing the glove library\n",
        "  corpus = Corpus() #training the corpus to generate the co occurence matrix which is used in GloVe\n",
        "  corpus.fit(input_data, window=size_of_window)#creating a Glove object which will use the matrix created in the above lines to create embeddings\n",
        "  #We can set the learning rate as it uses Gradient Descent and number of components\n",
        "  glove = Glove(no_components=vec_dims, learning_rate=alpha_) \n",
        "  glove.fit(corpus.matrix, epochs=num_epochs, no_threads=4, verbose=True)\n",
        "  glove.add_dictionary(corpus.dictionary)\n",
        "  glove.save(save_loc)\n",
        "\n",
        "def fast_text(input_data,save_loc,vec_dims,SG,size_of_window,minimum_count,no_workers,alpha_=0.025):\n",
        "  model = FastText(min_count=minimum_count, alpha=alpha_, size= vec_dims, workers=no_workers, window =size_of_window)\n",
        "  model.build_vocab(input_data)\n",
        "  model.train(input_data, epochs=model.epochs, total_examples=model.corpus_count, total_words=model.corpus_total_words)\n",
        "  model.save(save_loc)\n",
        "\n",
        "def hashing(input_data,vec_dims,save_loc):\n",
        "  model = HashingVectorizer(n_features=vec_dims)\n",
        "  model.transform(input_data)\n",
        "  #vectors = model.toarray()\n",
        "  #vocab = model.get_feature_names()\n",
        "  save_object(obj=model, filename=save_loc)\n",
        "\n",
        "def co_occur(input_data, save_loc, matrix_loc, maximum_features):\n",
        "  model = CountVectorizer(ngram_range=(1,1),max_features=maximum_features, token_pattern= r\"(?u)\\b\\w+\\b\")\n",
        "  X = model.fit_transform(input_data)\n",
        "  Xc = (X.T * X)\n",
        "  Xc.setdiag(0)\n",
        "  #cooccur = Xc.todense()\n",
        "  names = model.get_feature_names() # This are the entity names (i.e. keywords)\n",
        "  save_object(obj=names, filename='/content/drive/My Drive/Movielensdata/ml25m/cooccur/vocab')\n",
        "  df = pd.DataFrame(data = Xc.toarray(), columns = names, index = names)\n",
        "  save_object(obj=model, filename=save_loc)\n",
        "  save_object(obj=df, filename=matrix_loc)\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXlM6wLvQVIW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scoring_module(name,model_loc,data_loc,dict_loc,matrix_location):\n",
        "  data = load_object(data_loc)\n",
        "  if name == 'word2vec':\n",
        "    scores_list = score_word2vec(model_location=model_loc,input_data=data)\n",
        "  elif name == 'doc2vec':\n",
        "    scores_list = score_doc2vec(model_location=model_loc,input_data=data)\n",
        "  elif name == 'lsi':\n",
        "    scores_list = score_lsi(model_location=model_loc,input_data=data, dict_location=dict_loc)\n",
        "  elif name == 'tfidf':\n",
        "    scores_list = score_tfidf(model_location=model_loc,input_data=data, dict_location=dict_loc)\n",
        "  elif name == 'glove':\n",
        "    scores_list = score_glove(model_location=model_loc, input_data=data)\n",
        "  elif name == 'hashing':\n",
        "    scores_list = score_hashing(model_location=model_loc, input_data=data)\n",
        "  elif name == 'cooccur':\n",
        "    scores_list = score_cooccur(matrix_loc=matrix_location, input_data=data)\n",
        "  #elif name == 'fasttext':\n",
        "   # scores_list = score_fasttext(model_location=model_loc, input_data=data)\n",
        "  \n",
        "  return scores_list\n",
        "\n",
        "def score_word2vec(model_location,input_data):\n",
        "  model = Word2Vec.load(model_location)\n",
        "  scored_data = [model.wv[text] for text in input_data]\n",
        "  #print(model.wv['please'])\n",
        "  return scored_data\n",
        "\n",
        "def score_doc2vec(model_location,input_data):\n",
        "  model = Doc2Vec.load(model_location)\n",
        "  scored_data = [model.infer_vector(text) for text in input_data]\n",
        "  return scored_data\n",
        "\n",
        "def score_glove(model_location,input_data):\n",
        "  model = Glove.load(model_location)\n",
        "  scored_data = []\n",
        "  #doc = input_data[0]\n",
        "  for doc in input_data:\n",
        "    list1 = []\n",
        "    for text in doc:\n",
        "      list1.append(model.word_vectors[model.dictionary[text]])\n",
        "    scored_data.append(list1)\n",
        "  #scored_data = [model.word_vectors[model.dictionary[text]] for doc in input_data for text in doc]\n",
        "  return scored_data\n",
        "\n",
        "def score_fasttext(model_location, input_data):\n",
        "  model = FastText(model_location)\n",
        "  scored_data = [model[text] for text in input_data]\n",
        "  return scored_data\n",
        "\n",
        "def score_lsi(model_location, input_data, dict_location):\n",
        "  model = LsiModel.load(model_location)\n",
        "  dictionary = corpora.Dictionary.load(dict_location)\n",
        "  scored_data = [model[dictionary.doc2bow(text)] for text in input_data]\n",
        "  scored_dat = np.zeros((len(scored_data),len(scored_data[0])))\n",
        "  for i in range(len(scored_data)):\n",
        "    for k in range(len(scored_data[i])):\n",
        "      scored_dat[i][k] = scored_data[i][k][1]\n",
        "  return scored_dat\n",
        "\n",
        "def score_tfidf(model_location, input_data, dict_location):\n",
        "  model = TfidfModel.load(model_location)\n",
        "  dictionary = corpora.Dictionary.load(dict_location)\n",
        "  #print(dictionary)\n",
        "  scored_data = [model[dictionary.doc2bow(text)] for text in input_data]\n",
        "  scored_data_ = gensim.matutils.corpus2dense(scored_data, num_terms=len(dictionary), num_docs=len(scored_data))\n",
        "  #scored_data_ = gensim.matutils.corpus2csc(scored_data, num_terms=len(dictionary))\n",
        "  #scored_data_= np.zeros((len(scored_data), len(dictionary)), dtype=np.float64)\n",
        "  #for i in range(len(scored_data)):\n",
        "    #print(scored_data[i][k][0])\n",
        "   # for k in range(len(scored_data[i])):\n",
        "    #  scored_data_[i][scored_data[i][k][0]] = scored_data[i][k][1]\n",
        "  #scored_data = np.array(scored_data)\n",
        "  #scored_data = scored_data.transpose()\n",
        "  #scored_data_ = gensim.matutils.corpus2dense(scored_data, num_terms=len(dictionary), num_docs=len(scored_data))\n",
        "  #for i in range(len(scored_data)):\n",
        "   # scored_data_.append(scored_data[i].todense())\n",
        "  #tfidf_token= np.zeros((len(scored_data), 50), dtype=np.float64)\n",
        "  #tfidf_vals= np.zeros((len(scored_data), 50), dtype=np.float64)\n",
        "  #for i in range(len(input_data)):\n",
        "   # for k in range(len(scored_data[i])):\n",
        "    #  print(tfidf_vals[i][k])\n",
        "      #tfidf_token[i][k]=(list(scored_data))[i][k][0]\n",
        "      #tfidf_vals[i][k]=scored_data[i][k][1]\n",
        "  #tfidf_list=scored_data\n",
        "  #print(list(tfidf_data))\n",
        "  #print(tfidf_token)\n",
        "  #print(tfidf_vals)\n",
        "  return scored_data_, scored_data\n",
        "\n",
        "def score_hashing(model_location, input_data):\n",
        "  corpus = [str(str(doc)[1:-1]) for doc in input_data]\n",
        "  model = load_object(model_location)\n",
        "  X = model.transform(corpus)\n",
        "  Y = X.toarray()\n",
        "  #Doc_Term_Matrix = pd.DataFrame(X.toarray())\n",
        "  #Doc_Term_Matrix\n",
        "  return Y\n",
        "\n",
        "def score_cooccur(matrix_loc, input_data):\n",
        "  cooccur_matrix = load_object(matrix_loc)\n",
        "  #print(cooccur_matrix)\n",
        "  scored_data = [cooccur_matrix.loc[text].tolist() for doc in input_data for text in doc]\n",
        "  return scored_data"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgR527dPZlEc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sent_embedding(name,score_list):\n",
        "  if (name=='word2vec'):\n",
        "    sent_list = []\n",
        "    for i in range(len(score_list)):\n",
        "      sum = np.zeros((10))\n",
        "      for j in range(len(score_list[i])):\n",
        "        sum = sum + score_list[i][j]\n",
        "      sum = sum/(len(score_list[i]))\n",
        "      sent_list.append(sum)\n",
        "    sent_tensor = tf.convert_to_tensor(sent_list, dtype=tf.float32)\n",
        "  elif name=='glove':\n",
        "    sent_list = []\n",
        "    for i in range(len(score_list)):\n",
        "      sum = np.zeros((10))\n",
        "      for j in range(len(score_list[i])):\n",
        "        sum = sum+score_list[i][j]\n",
        "      sum = sum/(len(score_list[i]))\n",
        "      sent_list.append(sum)\n",
        "    sent_tensor = tf.convert_to_tensor(sent_list, dtype=tf.float32)\n",
        "  elif name=='doc2vec':\n",
        "    sent_tensor = tf.convert_to_tensor(score_list, dtype=tf.float32)\n",
        "  elif name=='tfidf':\n",
        "    sent_tensor = tf.convert_to_tensor(score_list, dtype=tf.float32)\n",
        "  elif name=='lsi':\n",
        "    sent_tensor = tf.convert_to_tensor(score_list, dtype=tf.float32)\n",
        "  elif name=='hashing':\n",
        "    sent_tensor = tf.convert_to_tensor(score_list, dtype=tf.float32)\n",
        "  return sent_tensor\n",
        "\n",
        "def Model_build(X,Y,X_predict,Y_predict,save_location,batches_size=100):\n",
        "  ops.reset_default_graph()\n",
        "  Model = Sequential()\n",
        "  #Model.add(Dense(20, activation='relu'))\n",
        "  #Model.add(Dropout(0.3))\n",
        "  Model.add(Dense(5, activation='relu'))\n",
        "  #Model.add(Dropout(0.3))\n",
        "  Model.add(Dense(1, activation='sigmoid'))\n",
        "  #Model.add(Dense(1))\n",
        "  #Model.add(Activation('softmax'))\n",
        "\n",
        "  opt = optimizers.Adam(lr=0.005, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "\n",
        "  Model.compile(loss='binary_crossentropy',optimizer=opt,metrics=['accuracy'])\n",
        "  #Model.summary()\n",
        "\n",
        "  steps_test = int(len(X)/batches_size)\n",
        "  steps_val = int(len(X_predict)/batches_size)\n",
        "\n",
        "  Model.fit(X, Y, epochs=10, batch_size=batches_size)\n",
        "\n",
        "  #print('The train accuracies are as given below :')\n",
        "  #print(History.history['accuracy'])\n",
        "  Model.save('/content/imdb/model')\n",
        "  #print(Model.metrics_names)\n",
        "  sentiment_score = Model.predict(X_predict, batch_size=batches_size, steps=steps_val, workers=3)\n",
        "  metrics = Model.evaluate(x=X_predict, y=Y_predict, batch_size=batches_size, verbose=1, steps=steps_val, max_queue_size=10, return_dict=False)\n",
        "  accuracy = metrics[1]\n",
        "  loss = metrics[0]\n",
        "  save_object(sentiment_score,save_location)\n",
        "  return sentiment_score, accuracy"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTON8_HeWKAV",
        "colab_type": "text"
      },
      "source": [
        "1) Name can be 'word2vec' or 'doc2vec' or 'lsi' or 'tfidf' or 'glove' or 'cooccur'.\n",
        "\n",
        "2) location is the path to input .csv file\n",
        "\n",
        "3) output_location is the path to save the processed data\n",
        "\n",
        "4) dict_location is the path to dictionary of LSI or TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajnRU9EfWhmP",
        "colab_type": "text"
      },
      "source": [
        "Change name, output_location for all models and dict_location for LSI and TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXdIQWmIGFOG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#data_module(reviews=False,name='glove',max_seq_length=100,location='/content/drive/My Drive/Movielensdata/ml25m/ratings.csv',output_location='/content/drive/My Drive/Movielensdata/ml25m/glove/data', dict_location='/content/drive/My Drive/Movielensdata/ml25m/tfidf/dict')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaH5acqZYYI5",
        "colab_type": "text"
      },
      "source": [
        "Change name, Data_location and model_save_location for all models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iWoBWvRZwSD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#embedding_model(name='glove',Data_location='/content/drive/My Drive/Movielensdata/ml25m/glove/data',model_save_location='/content/drive/My Drive/Movielensdata/ml25m/glove/glv', matrix_location='/content/drive/My Drive/Movielensdata/ml25m/cooccur/matrix', vector_dims=10,Sg=1,size_window=3,topics=10,mini_count=1,num_workers=3,max_num_epochs = 10,alpha = 0.025,min_alpha=0.00025,dm=1,maxi_features=None)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WqzalHzc_lp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#scoring_input = [['899', '2161', '3949', '5878', '1175', '1237', '8154', '2843', '7365', '4422', '6016'],['1080', '3114', '3671', '2791', '1288', '1', '541', '2692', '7323', '8014', '6370', '4703', '5147']]\n",
        "#save_object(scoring_input,'/content/drive/My Drive/Movielensdata/ml25m/scoring_input')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vuSkxUwYf8J",
        "colab_type": "text"
      },
      "source": [
        "Change name, model_loc or all models and dict_loc for LSI and TF-IDF "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ry_oK4hcUcJb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#scored_list = scoring_module(name='glove', model_loc='/content/drive/My Drive/Movielensdata/ml25m/glove/glv', data_loc='/content/drive/My Drive/Movielensdata/ml25m/scoring_input', dict_loc='/content/drive/My Drive/Movielensdata/ml25m/tfidf/dict', matrix_location ='/content/drive/My Drive/Movielensdata/ml25m/cooccur/matrix' )"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gG0y_xjhUQly",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(scored_list)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uld4mvrZfVMm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = load_object('/content/imdb/data')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_4LF_ieeXvW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_module(reviews=True, name='word2vec',max_seq_length=100,location='/content/imdb/data',output_location='/content/imdb/train_sent', dict_location='/content/imdb/tfidf_dict')"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EsUKDzuXFFL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "be6ae79f-e5fe-4f6c-990f-ebc0c54e7103"
      },
      "source": [
        "embedding_model(name='word2vec',Data_location='/content/imdb/train_sent',model_save_location='/content/imdb/trained_model', matrix_location='/content/drive/My Drive/Movielensdata/ml25m/cooccur/matrix',maxi_features=25000, vector_dims=10,Sg=1,size_window=3,topics=10,mini_count=1,num_workers=3,max_num_epochs = 10,alpha = 0.025,min_alpha=0.00025,dm=1)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIR9yYbeb_ES",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "961ff1b3-6c75-4d84-8d1d-488c24cee041"
      },
      "source": [
        "scored_test= scoring_module(name='word2vec', model_loc='/content/imdb/trained_model', data_loc='/content/imdb/test_sent', dict_loc='/content/imdb/tfidf_dict', matrix_location ='/content/drive/My Drive/Movielensdata/ml25m/cooccur/matrix' )"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BbXZZXvcU1H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "2adfb251-5326-48db-8bf3-bd8cabeac802"
      },
      "source": [
        "scored_val = scoring_module(name='word2vec', model_loc='/content/imdb/trained_model', data_loc='/content/imdb/val_sent', dict_loc='/content/imdb/tfidf_dict', matrix_location ='/content/drive/My Drive/Movielensdata/ml25m/cooccur/matrix' )"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNy7nTLz3QD1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#score[0]\n",
        "#print(score[1])"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fF5a-aMEOv4f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1ecdc89b-c716-4fcc-ea5b-01ed1c75d5d2"
      },
      "source": [
        "#print(scored_val[0])\n",
        "print(len(scored_val))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWt3x1q-OizO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(scored_test[0])"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWEvzm4ncfeP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_tensor = sent_embedding(name='word2vec',score_list=scored_test)\n",
        "val_tensor = sent_embedding(name='word2vec',score_list=scored_val)\n",
        "\n",
        "Y_test = load_object('/content/imdb/Y_test')\n",
        "Y_val = load_object('/content/imdb/Y_val')"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sw-6lgVdPv37",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "27ac7973-dd6b-4ecc-804e-096de3b699e1"
      },
      "source": [
        "print(test_tensor.shape)\n",
        "print(val_tensor.shape)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(15000, 10)\n",
            "(10000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Krs-h734cvcF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "outputId": "c0be9487-b4df-4e37-8946-55609200cdb1"
      },
      "source": [
        "sentiment_scores, accuracy = Model_build(X=test_tensor, Y=Y_test, X_predict=val_tensor, Y_predict=Y_val,save_location='/content/imdb/sentiment_word2vec',batches_size=100)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "150/150 [==============================] - 0s 994us/step - loss: 0.6950 - accuracy: 0.5726\n",
            "Epoch 2/10\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.6508 - accuracy: 0.6550\n",
            "Epoch 3/10\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.6269 - accuracy: 0.6668\n",
            "Epoch 4/10\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.6174 - accuracy: 0.6682\n",
            "Epoch 5/10\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 0.6130 - accuracy: 0.6687\n",
            "Epoch 6/10\n",
            "150/150 [==============================] - 0s 994us/step - loss: 0.6123 - accuracy: 0.6711\n",
            "Epoch 7/10\n",
            "150/150 [==============================] - 0s 957us/step - loss: 0.6121 - accuracy: 0.6699\n",
            "Epoch 8/10\n",
            "150/150 [==============================] - 0s 951us/step - loss: 0.6106 - accuracy: 0.6705\n",
            "Epoch 9/10\n",
            "150/150 [==============================] - 0s 955us/step - loss: 0.6091 - accuracy: 0.6721\n",
            "Epoch 10/10\n",
            "150/150 [==============================] - 0s 972us/step - loss: 0.6102 - accuracy: 0.6713\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "INFO:tensorflow:Assets written to: /content/imdb/model/assets\n",
            "100/100 [==============================] - 0s 927us/step - loss: 0.6104 - accuracy: 0.6686\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJiPaOzUeI9P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "d6908e55-940d-40eb-c551-2a70ea6fac1d"
      },
      "source": [
        "sentiment_scores[0:10]"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.7544033 ],\n",
              "       [0.6944938 ],\n",
              "       [0.20898059],\n",
              "       [0.63125134],\n",
              "       [0.25758255],\n",
              "       [0.29301885],\n",
              "       [0.1977123 ],\n",
              "       [0.53893894],\n",
              "       [0.31051928],\n",
              "       [0.67285866]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkOnPLL1eKZR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2ebd5165-6a14-47b6-bf05-bcc8674d0068"
      },
      "source": [
        "Y_val[0:10]"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, 1, 0, 0, 0, 1, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7xAYo8SOjxp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b0dd1fb0-ccda-41a2-eae8-65a4f46de35e"
      },
      "source": [
        "accuracy"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6686000227928162"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8uteRu_QQOU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 28,
      "outputs": []
    }
  ]
}