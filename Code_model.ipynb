{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Code_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1wgpB7rU7rrk4BnoapdXtGyFmepqlRw6K",
      "authorship_tag": "ABX9TyNm29icvhAShMz4JRCKYbLp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dharani1999/Word-embedding-techniques/blob/master/Code_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTPPDaIkL-DE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "30a70abe-8175-4476-c405-08ac8cf598eb"
      },
      "source": [
        "import pickle\n",
        "import gensim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import nltk\n",
        "import time\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from gensim.parsing.preprocessing import stem_text\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "from gensim import corpora, models, similarities\n",
        "from gensim.models import Word2Vec, TfidfModel, LsiModel\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from gensim.models.fasttext import FastText\n",
        "\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorboard as tb\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Conv2D,MaxPooling2D,Activation,Dense,Flatten,Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.python.framework import ops\n",
        "#tf.gfile = tf.io.gfile\n",
        "#tf.io.gfile = tb.compat.tensorflow_stub.io.gfile\n",
        "\n",
        "import random\n",
        "import pathlib\n",
        "import matplotlib.pyplot as plt\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4XGZScCdZgS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "outputId": "9c1141d7-1848-4aa1-b783-da26bc371694"
      },
      "source": [
        "#!pip install git+https://github.com/huggingface/transformers\n",
        "!pip install glove_python\n",
        "from glove import Corpus, Glove# creating a corpus object"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting glove_python\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3e/79/7e7e548dd9dcb741935d031117f4bed133276c2a047aadad42f1552d1771/glove_python-0.1.0.tar.gz (263kB)\n",
            "\r\u001b[K     |█▎                              | 10kB 13.6MB/s eta 0:00:01\r\u001b[K     |██▌                             | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |███▊                            | 30kB 2.2MB/s eta 0:00:01\r\u001b[K     |█████                           | 40kB 2.5MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 61kB 2.2MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 71kB 2.4MB/s eta 0:00:01\r\u001b[K     |██████████                      | 81kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 92kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 102kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 112kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 122kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 133kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 143kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 153kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 163kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 174kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 184kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 194kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 204kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 215kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 225kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 235kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 245kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 256kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 266kB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from glove_python) (1.18.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from glove_python) (1.4.1)\n",
            "Building wheels for collected packages: glove-python\n",
            "  Building wheel for glove-python (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for glove-python: filename=glove_python-0.1.0-cp36-cp36m-linux_x86_64.whl size=700222 sha256=5b29ee9079aaf67c0f8a5d7ab09379e70202015f7947e71c0bab036829b69efe\n",
            "  Stored in directory: /root/.cache/pip/wheels/88/4b/6d/10c0d2ad32c9d9d68beec9694a6f0b6e83ab1662a90a089a4b\n",
            "Successfully built glove-python\n",
            "Installing collected packages: glove-python\n",
            "Successfully installed glove-python-0.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nrx8P2P32kT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_object(obj, filename):\n",
        "    with open(filename, 'wb') as output:  # Overwrites any existing file.\n",
        "        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "def load_object(filename):\n",
        "    with open(filename, 'rb') as input:\n",
        "        pickle_object = pickle.load(input)\n",
        "    return  pickle_object"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtHGs7qlVUsn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        },
        "outputId": "76349f06-bf6c-4f2c-dd4d-bdc6527f005c"
      },
      "source": [
        "!mkdir imdb\n",
        "\n",
        "def pre_process(data):\n",
        "  _sent=[]\n",
        "  for i in data:\n",
        "    sent = word_tokenize(i)\n",
        "    sent = [w.lower() for w in sent]\n",
        "    #sent = [word for word in sent if word.isalpha()]\n",
        "    sent = [c for c in sent if not c.isdigit()]\n",
        "    sent = [stem_text(w) for w in sent]\n",
        "    #sent = [wordnet_lemmatizer.lemmatize(w) for w in sent]\n",
        "    #sent = [word for word in sent if word not in stoplist]\n",
        "    _sent.append(sent)\n",
        "  return _sent\n",
        "\n",
        "from keras.datasets import imdb\n",
        "data = imdb.load_data(path='/content/imdb/data',\n",
        "    num_words=None,\n",
        "    skip_top=0,\n",
        "    maxlen=None,\n",
        "    seed=113,\n",
        "    start_char=1,\n",
        "    oov_char=2,\n",
        "    index_from=3)\n",
        "save_object(data, '/content/imdb/data')\n",
        "word_to_idx_dict = imdb.get_word_index(path=\"/content/imdb/imdb_word_index.json\")\n",
        "n = len(word_to_idx_dict)\n",
        "print(\"Number distinct words in reviews = %d \\n\" % n)\n",
        "\n",
        "words_freq_list = []\n",
        "for (k,v) in word_to_idx_dict.items():\n",
        "  words_freq_list.append((k,v))\n",
        "\n",
        "sorted_list = sorted(words_freq_list, key=lambda x: x[1])\n",
        "\n",
        "print(\"Ten most common words: \\n\")\n",
        "print(sorted_list[0:10])\n",
        "\n",
        "print(\"\\nLast five least common words: \\n\")\n",
        "print(sorted_list[-5:])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n",
            "Number distinct words in reviews = 88584 \n",
            "\n",
            "Ten most common words: \n",
            "\n",
            "[('the', 1), ('and', 2), ('a', 3), ('of', 4), ('to', 5), ('is', 6), ('br', 7), ('in', 8), ('it', 9), ('i', 10)]\n",
            "\n",
            "Last five least common words: \n",
            "\n",
            "[(\"pipe's\", 88580), ('copywrite', 88581), ('artbox', 88582), (\"voorhees'\", 88583), (\"'l'\", 88584)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZszdGlsnK4V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "outputId": "9229b899-b1f9-4f61-abf5-8f8b78b98b73"
      },
      "source": [
        "d = load_object('/content/imdb/data')\n",
        "d"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((array([list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]),\n",
              "         list([1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 10156, 4, 1153, 9, 194, 775, 7, 8255, 11596, 349, 2637, 148, 605, 15358, 8003, 15, 123, 125, 68, 23141, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 36893, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 25249, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 46151, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]),\n",
              "         list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 44076, 780, 8, 106, 14, 6905, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 51428, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113]),\n",
              "         ...,\n",
              "         list([1, 11, 6, 230, 245, 6401, 9, 6, 1225, 446, 86527, 45, 2174, 84, 8322, 4007, 21, 4, 912, 84, 14532, 325, 725, 134, 15271, 1715, 84, 5, 36, 28, 57, 1099, 21, 8, 140, 8, 703, 5, 11656, 84, 56, 18, 1644, 14, 9, 31, 7, 4, 9406, 1209, 2295, 26094, 1008, 18, 6, 20, 207, 110, 563, 12, 8, 2901, 17793, 8, 97, 6, 20, 53, 4767, 74, 4, 460, 364, 1273, 29, 270, 11, 960, 108, 45, 40, 29, 2961, 395, 11, 6, 4065, 500, 7, 14492, 89, 364, 70, 29, 140, 4, 64, 4780, 11, 4, 2678, 26, 178, 4, 529, 443, 17793, 5, 27, 710, 117, 74936, 8123, 165, 47, 84, 37, 131, 818, 14, 595, 10, 10, 61, 1242, 1209, 10, 10, 288, 2260, 1702, 34, 2901, 17793, 4, 65, 496, 4, 231, 7, 790, 5, 6, 320, 234, 2766, 234, 1119, 1574, 7, 496, 4, 139, 929, 2901, 17793, 7750, 5, 4241, 18, 4, 8497, 13164, 250, 11, 1818, 7561, 4, 4217, 5408, 747, 1115, 372, 1890, 1006, 541, 9303, 7, 4, 59, 11027, 4, 3586, 22459]),\n",
              "         list([1, 1446, 7079, 69, 72, 3305, 13, 610, 930, 8, 12, 582, 23, 5, 16, 484, 685, 54, 349, 11, 4120, 2959, 45, 58, 1466, 13, 197, 12, 16, 43, 23, 21469, 5, 62, 30, 145, 402, 11, 4131, 51, 575, 32, 61, 369, 71, 66, 770, 12, 1054, 75, 100, 2198, 8, 4, 105, 37, 69, 147, 712, 75, 3543, 44, 257, 390, 5, 69, 263, 514, 105, 50, 286, 1814, 23, 4, 123, 13, 161, 40, 5, 421, 4, 116, 16, 897, 13, 40691, 40, 319, 5872, 112, 6700, 11, 4803, 121, 25, 70, 3468, 4, 719, 3798, 13, 18, 31, 62, 40, 8, 7200, 4, 29455, 7, 14, 123, 5, 942, 25, 8, 721, 12, 145, 5, 202, 12, 160, 580, 202, 12, 6, 52, 58, 11418, 92, 401, 728, 12, 39, 14, 251, 8, 15, 251, 5, 21213, 12, 38, 84, 80, 124, 12, 9, 23]),\n",
              "         list([1, 17, 6, 194, 337, 7, 4, 204, 22, 45, 254, 8, 106, 14, 123, 4, 12815, 270, 14437, 5, 16923, 12255, 732, 2098, 101, 405, 39, 14, 1034, 4, 1310, 9, 115, 50, 305, 12, 47, 4, 168, 5, 235, 7, 38, 111, 699, 102, 7, 4, 4039, 9245, 9, 24, 6, 78, 1099, 17, 2345, 16553, 21, 27, 9685, 6139, 5, 29043, 1603, 92, 1183, 4, 1310, 7, 4, 204, 42, 97, 90, 35, 221, 109, 29, 127, 27, 118, 8, 97, 12, 157, 21, 6789, 85010, 9, 6, 66, 78, 1099, 4, 631, 1191, 5, 2642, 272, 191, 1070, 6, 7585, 8, 2197, 70907, 10755, 544, 5, 383, 1271, 848, 1468, 12183, 497, 16876, 8, 1597, 8778, 19280, 21, 60, 27, 239, 9, 43, 8368, 209, 405, 10, 10, 12, 764, 40, 4, 248, 20, 12, 16, 5, 174, 1791, 72, 7, 51, 6, 1739, 22, 4, 204, 131, 9])],\n",
              "        dtype=object), array([1, 0, 0, ..., 0, 1, 0])),\n",
              " (array([list([1, 591, 202, 14, 31, 6, 717, 10, 10, 18142, 10698, 5, 4, 360, 7, 4, 177, 5760, 394, 354, 4, 123, 9, 1035, 1035, 1035, 10, 10, 13, 92, 124, 89, 488, 7944, 100, 28, 1668, 14, 31, 23, 27, 7479, 29, 220, 468, 8, 124, 14, 286, 170, 8, 157, 46, 5, 27, 239, 16, 179, 15387, 38, 32, 25, 7944, 451, 202, 14, 6, 717]),\n",
              "         list([1, 14, 22, 3443, 6, 176, 7, 5063, 88, 12, 2679, 23, 1310, 5, 109, 943, 4, 114, 9, 55, 606, 5, 111, 7, 4, 139, 193, 273, 23, 4, 172, 270, 11, 7216, 10626, 4, 8463, 2801, 109, 1603, 21, 4, 22, 3861, 8, 6, 1193, 1330, 10, 10, 4, 105, 987, 35, 841, 16873, 19, 861, 1074, 5, 1987, 17975, 45, 55, 221, 15, 670, 5304, 526, 14, 1069, 4, 405, 5, 2438, 7, 27, 85, 108, 131, 4, 5045, 5304, 3884, 405, 9, 3523, 133, 5, 50, 13, 104, 51, 66, 166, 14, 22, 157, 9, 4, 530, 239, 34, 8463, 2801, 45, 407, 31, 7, 41, 3778, 105, 21, 59, 299, 12, 38, 950, 5, 4521, 15, 45, 629, 488, 2733, 127, 6, 52, 292, 17, 4, 6936, 185, 132, 1988, 5304, 1799, 488, 2693, 47, 6, 392, 173, 4, 21686, 4378, 270, 2352, 4, 1500, 7, 4, 65, 55, 73, 11, 346, 14, 20, 9, 6, 976, 2078, 7, 5293, 861, 12746, 5, 4182, 30, 3127, 23651, 56, 4, 841, 5, 990, 692, 8, 4, 1669, 398, 229, 10, 10, 13, 2822, 670, 5304, 14, 9, 31, 7, 27, 111, 108, 15, 2033, 19, 7836, 1429, 875, 551, 14, 22, 9, 1193, 21, 45, 4829, 5, 45, 252, 8, 12508, 6, 565, 921, 3639, 39, 4, 529, 48, 25, 181, 8, 67, 35, 1732, 22, 49, 238, 60, 135, 1162, 14, 9, 290, 4, 58, 10, 10, 472, 45, 55, 878, 8, 169, 11, 374, 5687, 25, 203, 28, 8, 818, 12, 125, 4, 3077]),\n",
              "         list([1, 111, 748, 4368, 1133, 33782, 24563, 4, 87, 1551, 1262, 7, 31, 318, 9459, 7, 4, 498, 5076, 748, 63, 29, 5161, 220, 686, 10941, 5, 17, 12, 575, 220, 2507, 17, 6, 185, 132, 24563, 16, 53, 928, 11, 51278, 74, 4, 438, 21, 27, 10044, 589, 8, 22, 107, 20123, 19550, 997, 1638, 8, 35, 2076, 9019, 11, 22, 231, 54, 29, 1706, 29, 100, 18995, 2425, 34, 12998, 8738, 48078, 5, 19353, 98, 31, 2122, 33, 6, 58, 14, 3808, 1638, 8, 4, 365, 7, 2789, 3761, 356, 346, 4, 27608, 1060, 63, 29, 93, 11, 5421, 11, 15236, 33, 6, 58, 54, 1270, 431, 748, 7, 32, 2580, 16, 11, 94, 19469, 10, 10, 4, 993, 45222, 7, 4, 1766, 2634, 2164, 24563, 8, 847, 8, 1450, 121, 31, 7, 27, 86, 2663, 10760, 16, 6, 465, 993, 2006, 30995, 573, 17, 61862, 42, 4, 17345, 37, 473, 6, 711, 6, 8869, 7, 328, 212, 70, 30, 258, 11, 220, 32, 7, 108, 21, 133, 12, 9, 55, 465, 849, 3711, 53, 33, 2071, 1969, 37, 70, 1144, 4, 5940, 1409, 74, 476, 37, 62, 91, 1329, 169, 4, 1330, 10104, 146, 655, 2212, 5, 258, 12, 184, 10104, 546, 5, 849, 10333, 7, 4, 22, 1436, 18, 631, 1386, 797, 7, 4, 8712, 71, 348, 425, 4320, 1061, 19, 10288, 5, 12141, 11, 661, 8, 339, 17863, 4, 2455, 11434, 7, 4, 1962, 10, 10, 263, 787, 9, 270, 11, 6, 9466, 4, 61862, 48414, 121, 4, 5437, 26, 4434, 19, 68, 1372, 5, 28, 446, 6, 318, 7149, 8, 67, 51, 36, 70, 81, 8, 4392, 2294, 36, 1197, 8, 68411, 25399, 18, 6, 711, 4, 9909, 26, 10296, 1125, 11, 14, 636, 720, 12, 426, 28, 77, 776, 8, 97, 38, 111, 7489, 6175, 168, 1239, 5189, 137, 25399, 18, 27, 173, 9, 2399, 17, 6, 12397, 428, 14657, 232, 11, 4, 8014, 37, 272, 40, 2708, 247, 30, 656, 6, 13182, 54, 25399, 3292, 98, 6, 2840, 40, 558, 37, 6093, 98, 4, 17345, 1197, 15, 14, 9, 57, 4893, 5, 4659, 6, 275, 711, 7937, 25399, 3292, 98, 6, 31036, 10, 10, 6639, 19, 14, 10241, 267, 162, 711, 37, 5900, 752, 98, 4, 17345, 2378, 90, 19, 6, 73284, 7, 36744, 1810, 77553, 4, 4770, 3183, 930, 8, 508, 90, 4, 1317, 8, 4, 48414, 17, 15454, 3965, 1853, 4, 1494, 8, 4468, 189, 4, 31036, 6287, 5774, 4, 4770, 5, 95, 271, 23, 6, 7742, 6063, 21627, 5437, 33, 1526, 6, 425, 3155, 33697, 4535, 1636, 7, 4, 4669, 11966, 469, 4, 4552, 54, 4, 150, 5664, 17345, 280, 53, 68411, 25399, 18, 339, 29, 1978, 27, 7885, 5, 17303, 68, 1830, 19, 6571, 14605, 4, 1515, 7, 263, 65, 2132, 34, 6, 5680, 7489, 43, 159, 29, 9, 4706, 9, 387, 73, 195, 584, 10, 10, 1069, 4, 58, 810, 54, 14, 6078, 117, 22, 16, 93, 5, 1069, 4, 192, 15, 12, 16, 93, 34, 6, 1766, 28228, 33, 4, 5673, 7, 15, 18760, 9252, 3286, 325, 12, 62, 30, 776, 8, 67, 14, 17, 6, 12214, 44, 148, 687, 24563, 203, 42, 203, 24, 28, 69, 32157, 6676, 11, 330, 54, 29, 93, 61862, 21, 845, 14148, 27, 1099, 7, 819, 4, 22, 1407, 17, 6, 14967, 787, 7, 2460, 19569, 61862, 100, 30, 4, 3737, 3617, 3169, 2321, 42, 1898, 11, 4, 3814, 42, 101, 704, 7, 101, 999, 15, 1625, 94, 2926, 180, 5, 9, 9101, 34, 15205, 45, 6, 1429, 22, 60, 6, 1220, 31, 11, 94, 6408, 96, 21, 94, 749, 9, 57, 975]),\n",
              "         ...,\n",
              "         list([1, 13, 1408, 15, 8, 135, 14, 9, 35, 32, 46, 394, 20, 62, 30, 5093, 21, 45, 184, 78, 4, 1492, 910, 769, 2290, 2515, 395, 4257, 5, 1454, 11, 119, 16946, 89, 1036, 4, 116, 218, 78, 21, 407, 100, 30, 128, 262, 15, 7, 185, 2280, 284, 1842, 60664, 37, 315, 4, 226, 20, 272, 2942, 40, 29, 152, 60, 181, 8, 30, 50, 553, 362, 80, 119, 12, 21, 846, 5518]),\n",
              "         list([1, 11, 119, 241, 9, 4, 840, 20, 12, 468, 15, 94, 3684, 562, 791, 39, 4, 86, 107, 8, 97, 14, 31, 33, 4, 2960, 7, 743, 46, 1028, 9, 3531, 5, 4, 768, 47, 8, 79, 90, 145, 164, 162, 50, 6, 501, 119, 7, 9, 4, 78, 232, 15, 16, 224, 11, 4, 333, 20, 4, 985, 200, 5, 28739, 5, 9, 1861, 8, 79, 357, 4, 20, 47, 220, 57, 206, 139, 11, 12, 5, 55, 117, 212, 13, 1276, 92, 124, 51, 45, 1188, 71, 536, 13, 520, 14, 20, 6, 2302, 7, 470]),\n",
              "         list([1, 6, 52, 7465, 430, 22, 9, 220, 2594, 8, 28, 24357, 519, 3227, 6, 769, 15, 47, 6, 3482, 4067, 8, 114, 5, 33, 222, 31, 55, 184, 704, 5586, 18020, 19, 346, 3153, 5, 6, 364, 350, 4, 184, 5586, 9, 133, 1810, 11, 5417, 13226, 21, 4, 7298, 42657, 570, 50, 2005, 2643, 9, 6, 1249, 17, 6, 25194, 27803, 21, 17, 6, 1211, 232, 1138, 2249, 29, 266, 56, 96, 346, 194, 308, 9, 194, 21, 29, 218, 1078, 19, 4, 78, 173, 7, 27, 20067, 5698, 3406, 718, 21264, 9, 6, 6907, 17, 210, 5, 3281, 5677, 47, 77, 395, 14, 172, 173, 18, 2740, 2931, 4517, 82, 127, 27, 173, 11, 6, 392, 217, 21, 50, 9, 57, 65, 12, 14274, 53, 40, 35, 390, 7, 11, 4, 3567, 7, 4, 314, 74, 6, 792, 22, 16261, 19, 714, 727, 5205, 382, 4, 91, 6533, 439, 19, 14, 20, 9, 1441, 5805, 1118, 4, 756, 25, 124, 4, 31, 12, 16, 93, 804, 34, 2005, 2643])],\n",
              "        dtype=object),\n",
              "  array([0, 1, 1, ..., 0, 0, 0])))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EslXGch8IZgd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_module(reviews, name, location, output_location, dict_location, max_seq_length=100):\n",
        "  if reviews==False:\n",
        "    dataset1 = pd.read_csv(location)\n",
        "    dataset = dataset1.iloc[0:1000,:]\n",
        "  else:\n",
        "    dataset = load_object(location)\n",
        "  if name == 'word2vec':\n",
        "    data_corpus, users_total = data_word2vec(reviews,dataset,max_seq_length)\n",
        "  elif name == 'doc2vec':\n",
        "    data_corpus, users_total = data_doc2vec(reviews,dataset,max_seq_length)\n",
        "  elif name == 'lsi':\n",
        "    data_corpus, users_total = data_lsi(dataset,max_seq_length,dict_loc=dict_location)\n",
        "  elif name == 'tfidf':\n",
        "    data_corpus, users_total = data_lsi(dataset,max_seq_length,dict_loc=dict_location)\n",
        "  elif name == 'glove':\n",
        "    data_corpus, users_total = data_word2vec(dataset,max_seq_length)\n",
        "  elif name == 'hashing':\n",
        "    data_corpus, users_total = data_hashing(dataset,max_seq_length)\n",
        "  elif name == 'cooccur':\n",
        "    data_corpus, users_total = data_hashing(dataset,max_seq_length)\n",
        "  #elif name == 'fasttext':\n",
        "   # data_corpus, users_total = data_word2vec(dataset,max_seq_length)\n",
        "  #data_corpus.save('/content/drive/My Drive/Movielensdata/ml25m/data_corpus')\n",
        "  #print(data_corpus)\n",
        "  save_object(obj=data_corpus, filename=output_location)\n",
        "  #return data_corpus, users_total\n",
        "\n",
        "def data_word2vec(review,dataset,max_seq_length):\n",
        "  if review==False:\n",
        "    dataset.sort_values(by=['userId','timestamp'],inplace=True)\n",
        "    user_total = len(dataset['userId'].unique())\n",
        "    \n",
        "    #Selecting the most recent movies rated by each user and padding if necessary\n",
        "    movie_list = []\n",
        "    for i in range(user_total):\n",
        "      list1 = []\n",
        "      list1 = dataset.loc[dataset['userId'] ==(i+1),['movieId']]['movieId'].tolist()\n",
        "      if len(list1)>max_seq_length:\n",
        "        list1 = list1[(len(list1)-max_seq_length):]\n",
        "      elif len(list1)<max_seq_length:\n",
        "        list1 = list1+[0 for j in range((max_seq_length-len(list1)))]\n",
        "        #for j in range((max_seq_length-len(list1))):\n",
        "        # list1.append(0)\n",
        "      movie_list.append(list1)\n",
        "\n",
        "    #Selecting the most recent ratings rated by each user and padding if necessary\n",
        "    rating_list =[]\n",
        "    for i in range(user_total):\n",
        "      list2 = []\n",
        "      list2 = dataset.loc[dataset['userId'] ==(i+1),['rating']]['rating'].tolist()\n",
        "      if len(list2)>max_seq_length:\n",
        "        list2 = list2[(len(list2)-max_seq_length):]\n",
        "      elif len(list2)<max_seq_length:\n",
        "        list2 = list2+[0 for j in range((max_seq_length-len(list2)))]\n",
        "        #for j in range((max_seq_length-len(list2))):\n",
        "        # list2.append(0)\n",
        "      rating_list.append(list2)\n",
        "\n",
        "    #Creating user_id level transpose matrices\n",
        "    movies_transpose = pd.DataFrame(data=movie_list,index=[i+1 for i in range(user_total)])\n",
        "    movies_transpose.index.names = ['userId']\n",
        "    #print(movies_transpose)\n",
        "\n",
        "    ratings_transpose = pd.DataFrame(data=rating_list,index=[i+1 for i in range(user_total)])\n",
        "    ratings_transpose.index.names = ['userId']\n",
        "    #print(ratings_transpose)\n",
        "\n",
        "    # Select features from original dataset to form a new dataframe \n",
        "    df1 = movies_transpose.iloc[:]# For each row, combine all the columns into one column\n",
        "    df2 = df1.apply(lambda x: ','.join(x.astype(str)), axis=1)# Store them in a pandas dataframe\n",
        "    df_clean = pd.DataFrame({'clean': df2})# Create the list of list format of the custom corpus for gensim modeling \n",
        "    sent = [row.split(',') for row in df_clean['clean']]\n",
        "\n",
        "    return sent, user_total\n",
        "  else:\n",
        "    X_train = dataset[0][0]\n",
        "    Y_train = dataset[0][1]\n",
        "    X_test, X_val, Y_test, Y_val = train_test_split(data[1][0], data[1][1], test_size=0.4, shuffle=False)\n",
        "    \n",
        "    reverse_word_index = dict([(value, key) for (key, value) in word_to_idx_dict.items()])\n",
        "    train_reviews=[]\n",
        "    for j in X_train:\n",
        "      train_reviews.append(' '.join([reverse_word_index.get(i - 3, '') for i in j]))\n",
        "\n",
        "    test_reviews=[]\n",
        "    for j in X_test:\n",
        "      test_reviews.append(' '.join([reverse_word_index.get(i - 3, '') for i in j]))\n",
        "\n",
        "    val_reviews=[]\n",
        "    for j in X_val:\n",
        "      val_reviews.append(' '.join([reverse_word_index.get(i - 3, '') for i in j]))\n",
        "\n",
        "    train_sent=pre_process(train_reviews)\n",
        "    save_object(train_sent,'/content/imdb/train_sent')\n",
        "    test_sent=pre_process(test_reviews)\n",
        "    save_object(test_sent,'/content/imdb/test_sent')\n",
        "    val_sent=pre_process(val_reviews)\n",
        "    save_object(test_sent,'/content/imdb/val_sent')\n",
        "    save_object(Y_train, '/content/imdb/Y_train')\n",
        "    save_object(Y_test, '/content/imdb/Y_test')\n",
        "    save_object(Y_val, '/content/imdb/Y_val')\n",
        "\n",
        "    return train_sent, len(train_sent)\n",
        "\n",
        "def data_doc2vec(review,dataset,max_seq_length):\n",
        "  Sent, user_total = data_word2vec(review,dataset,max_seq_length)\n",
        "  tagged_data = []\n",
        "  tags = []\n",
        "  \n",
        "  for i in range(user_total):\n",
        "    tagged_data = tagged_data + [TaggedDocument(words=Sent[i], tags=[str(i)])]\n",
        "\n",
        "  return tagged_data, user_total\n",
        "\n",
        "def data_lsi(dataset,max_seq_length,dict_loc):\n",
        "  Sent, user_total = data_word2vec(dataset,max_seq_length)\n",
        "  dictionary = corpora.Dictionary(Sent)\n",
        "  #print(dictionary.token2id)\n",
        "  corpus = [dictionary.doc2bow(text) for text in Sent]\n",
        "  dictionary.save(dict_loc)\n",
        "  #corpus = np.array([[(id, freq) for id, freq in cp] for cp in corp])\n",
        "  #corpus = gensim.matutils.Dense2Corpus(np.array(Sent),documents_columns=False)\n",
        "\n",
        "  return corpus, user_total\n",
        "\n",
        "def data_hashing(dataset,max_seq_length):\n",
        "  Sent, user_total = data_word2vec(dataset,max_seq_length)\n",
        "  corpus = [str(str(doc)[1:-1]) for doc in Sent]\n",
        "  return corpus, user_total"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kJytoDNK3E0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def embedding_model(name,Data_location,model_save_location,matrix_location, vector_dims=10,Sg=1,size_window=3,topics=10,mini_count=1,num_workers=3,max_num_epochs = 100,alpha = 0.025,min_alpha=0.00025,dm=1, maxi_features=None):\n",
        "  Data = load_object(Data_location)\n",
        "  #print(Data)\n",
        "  if name == 'word2vec':\n",
        "    word2vec(input_data=Data, save_loc=model_save_location, vec_dims=vector_dims, SG=Sg, size_of_window=size_window, minimum_count=mini_count, no_workers=num_workers)\n",
        "    #voc = model1.wv\n",
        "    #words = list(model1.wv.vocab)\n",
        "    #vectors = model1[model1.wv.vocab]\n",
        "  elif name == 'doc2vec':\n",
        "    doc2vec(input_data=Data, save_loc=model_save_location, vec_dims=vector_dims, alpha_=alpha, size_of_window=size_window, no_workers=num_workers, max_epochs=max_num_epochs,min_alpha_=min_alpha, minimum_count=mini_count, dms=dm)\n",
        "  elif name == 'lsi':\n",
        "    lsi(input_data=Data, save_loc=model_save_location, total_topics=topics)\n",
        "  elif name == 'tfidf':\n",
        "    tfidf(input_data=Data,save_loc=model_save_location)\n",
        "  elif name == 'glove':\n",
        "    glove_model(input_data=Data, vec_dims=vector_dims, size_of_window=size_window, save_loc=model_save_location, num_epochs=max_num_epochs, alpha_=0.05, num_threads=4)\n",
        "  elif name == 'hashing':\n",
        "    hashing(input_data=Data, vec_dims=vector_dims, save_loc=model_save_location)\n",
        "  elif name == 'cooccur':\n",
        "    co_occur(input_data=Data, maximum_features=maxi_features, save_loc=model_save_location, matrix_loc = matrix_location)\n",
        "  #elif name == 'fasttext':\n",
        "   # fast_text(input_data=Data, save_loc=model_save_location, vec_dims=vector_dims, SG=Sg, size_of_window=size_window, minimum_count=mini_count, no_workers=num_workers, alpha_=0.025)\n",
        "\n",
        "def word2vec(input_data,save_loc,vec_dims,SG,size_of_window,minimum_count,no_workers):\n",
        "  model = Word2Vec(input_data,min_count=minimum_count,size= vec_dims,workers=no_workers, window =size_of_window, sg = SG)\n",
        "  model.save(save_loc)\n",
        "\n",
        "def doc2vec(input_data,save_loc,vec_dims,alpha_,size_of_window,min_alpha_,minimum_count,dms,no_workers,max_epochs):\n",
        "  model = Doc2Vec(size=vec_dims,\n",
        "                alpha=alpha_, \n",
        "                min_alpha=min_alpha_,\n",
        "                window = size_of_window,\n",
        "                min_count=minimum_count,\n",
        "                dm =dms)\n",
        "  model.build_vocab(input_data)\n",
        "\n",
        "  for epoch in range(max_epochs):\n",
        "    #print('iteration {0}'.format(epoch))\n",
        "    model.train(input_data, total_examples=model.corpus_count, epochs=model.iter)\n",
        "    # decrease the learning rate\n",
        "    model.alpha -= 0.0002\n",
        "    # fix the learning rate, no decay\n",
        "    model.min_alpha = model.alpha\n",
        "  model.save(save_loc)\n",
        "\n",
        "def lsi(input_data,save_loc,total_topics):\n",
        "  model = models.LsiModel(corpus=input_data, num_topics=total_topics)\n",
        "  index = similarities.MatrixSimilarity(model[input_data])\n",
        "  lsi_data = model[input_data]\n",
        "  lsi_topics = model.print_topics()\n",
        "  #for topic in lsi_topics:\n",
        "    #print(topic)\n",
        "  model.save(save_loc)\n",
        "\n",
        "def tfidf(input_data,save_loc):\n",
        "  model = models.TfidfModel(corpus=input_data)\n",
        "  tfidf_data = model[input_data]\n",
        "\n",
        "  tfidf_token= np.zeros((len(tfidf_data), 350), dtype=np.float64)\n",
        "  tfidf_vals= np.zeros((len(tfidf_data), 350), dtype=np.float64)\n",
        " \n",
        "  for i in range(len(input_data)):\n",
        "    for k in range(len(list(tfidf_data)[i])):\n",
        "      tfidf_token[i][k]=(list(tfidf_data))[i][k][0]\n",
        "      tfidf_vals[i][k]=(list(tfidf_data))[i][k][1]\n",
        "  tfidf_list=list(tfidf_data)\n",
        "  #print(list(tfidf_data))\n",
        "  model.save(save_loc)\n",
        "\n",
        "def glove_model(input_data,vec_dims,size_of_window,save_loc,alpha_=0.05,num_epochs=30, num_threads=4):\n",
        "  #importing the glove library\n",
        "  corpus = Corpus() #training the corpus to generate the co occurence matrix which is used in GloVe\n",
        "  corpus.fit(input_data, window=size_of_window)#creating a Glove object which will use the matrix created in the above lines to create embeddings\n",
        "  #We can set the learning rate as it uses Gradient Descent and number of components\n",
        "  glove = Glove(no_components=vec_dims, learning_rate=alpha_) \n",
        "  glove.fit(corpus.matrix, epochs=num_epochs, no_threads=4, verbose=True)\n",
        "  glove.add_dictionary(corpus.dictionary)\n",
        "  glove.save(save_loc)\n",
        "\n",
        "def fast_text(input_data,save_loc,vec_dims,SG,size_of_window,minimum_count,no_workers,alpha_=0.025):\n",
        "  model = FastText(min_count=minimum_count, alpha=alpha_, size= vec_dims, workers=no_workers, window =size_of_window)\n",
        "  model.build_vocab(input_data)\n",
        "  model.train(input_data, epochs=model.epochs, total_examples=model.corpus_count, total_words=model.corpus_total_words)\n",
        "  model.save(save_loc)\n",
        "\n",
        "def hashing(input_data,vec_dims,save_loc):\n",
        "  model = HashingVectorizer(n_features=vec_dims)\n",
        "  model.transform(input_data)\n",
        "  #vectors = model.toarray()\n",
        "  #vocab = model.get_feature_names()\n",
        "  save_object(obj=model, filename=save_loc)\n",
        "\n",
        "def co_occur(input_data, save_loc, matrix_loc, maximum_features):\n",
        "  model = CountVectorizer(ngram_range=(1,1),max_features=maximum_features, token_pattern= r\"(?u)\\b\\w+\\b\")\n",
        "  X = model.fit_transform(input_data)\n",
        "  Xc = (X.T * X)\n",
        "  Xc.setdiag(0)\n",
        "  #cooccur = Xc.todense()\n",
        "  names = model.get_feature_names() # This are the entity names (i.e. keywords)\n",
        "  save_object(obj=names, filename='/content/drive/My Drive/Movielensdata/ml25m/cooccur/vocab')\n",
        "  df = pd.DataFrame(data = Xc.toarray(), columns = names, index = names)\n",
        "  save_object(obj=model, filename=save_loc)\n",
        "  save_object(obj=df, filename=matrix_loc)\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXlM6wLvQVIW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scoring_module(name,model_loc,data_loc,dict_loc,matrix_location):\n",
        "  data = load_object(data_loc)\n",
        "  if name == 'word2vec':\n",
        "    scores_list = score_word2vec(model_location=model_loc,input_data=data)\n",
        "  elif name == 'doc2vec':\n",
        "    scores_list = score_doc2vec(model_location=model_loc,input_data=data)\n",
        "  elif name == 'lsi':\n",
        "    scores_list = score_lsi(model_location=model_loc,input_data=data, dict_location=dict_loc)\n",
        "  elif name == 'tfidf':\n",
        "    scores_list = score_tfidf(model_location=model_loc,input_data=data, dict_location=dict_loc)\n",
        "  elif name == 'glove':\n",
        "    scores_list = score_glove(model_location=model_loc, input_data=data)\n",
        "  elif name == 'hashing':\n",
        "    scores_list = score_hashing(model_location=model_loc, input_data=data)\n",
        "  elif name == 'cooccur':\n",
        "    scores_list = score_cooccur(matrix_loc=matrix_location, input_data=data)\n",
        "  #elif name == 'fasttext':\n",
        "   # scores_list = score_fasttext(model_location=model_loc, input_data=data)\n",
        "  \n",
        "  return scores_list\n",
        "\n",
        "def score_word2vec(model_location,input_data):\n",
        "  model = Word2Vec.load(model_location)\n",
        "  scored_data = [model.wv[text] for text in input_data]\n",
        "  #print(model.wv['please'])\n",
        "  return scored_data\n",
        "\n",
        "def score_doc2vec(model_location,input_data):\n",
        "  model = Doc2Vec.load(model_location)\n",
        "  scored_data = [model.infer_vector(text) for text in input_data]\n",
        "  return scored_data\n",
        "\n",
        "def score_glove(model_location,input_data):\n",
        "  model = Glove.load(model_location)\n",
        "  scored_data = [model.word_vectors[model.dictionary[text]] for doc in input_data for text in doc]\n",
        "  #model.word_vectors[]\n",
        "  return scored_data\n",
        "\n",
        "def score_fasttext(model_location, input_data):\n",
        "  model = FastText(model_location)\n",
        "  scored_data = [model[text] for text in input_data]\n",
        "  return scored_data\n",
        "\n",
        "def score_lsi(model_location, input_data, dict_location):\n",
        "  #model = LsiModel.load(model_location)\n",
        "  dictionary = corpora.Dictionary.load(dict_location)\n",
        "  scored_data = [model[dictionary.doc2bow(text)] for text in input_data]\n",
        "  return scored_data\n",
        "\n",
        "def score_tfidf(model_location, input_data, dict_location):\n",
        "  #model = TfidfModel.load(model_location)\n",
        "  dictionary = corpora.Dictionary.load(dict_location)\n",
        "  scored_data = [model[dictionary.doc2bow(text)] for text in input_data]\n",
        "  return scored_data\n",
        "\n",
        "def score_hashing(model_location, input_data):\n",
        "  corpus = [str(str(doc)[1:-1]) for doc in input_data]\n",
        "  model = load_object(model_location)\n",
        "  X = model.transform(corpus)\n",
        "  Doc_Term_Matrix = pd.DataFrame(X.toarray())\n",
        "  Doc_Term_Matrix\n",
        "  return Doc_Term_Matrix\n",
        "\n",
        "def score_cooccur(matrix_loc, input_data):\n",
        "  cooccur_matrix = load_object(matrix_loc)\n",
        "  #print(cooccur_matrix)\n",
        "  scored_data = [cooccur_matrix.loc[text].tolist() for doc in input_data for text in doc]\n",
        "  return scored_data"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgR527dPZlEc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sent_embedding(name,score_list):\n",
        "  if name=='word2vec':\n",
        "    sent_list = []\n",
        "    for i in range(len(score_list)):\n",
        "      sum = np.zeros((10))\n",
        "      for j in range(len(score_list[i])):\n",
        "        sum = sum + score_list[i][j]\n",
        "      sum = sum/(len(score_list[i]))\n",
        "      sent_list.append(sum)\n",
        "    sent_tensor = tf.convert_to_tensor(sent_list, dtype=tf.float32)\n",
        "  elif name=='doc2vec':\n",
        "    sent_tensor = tf.convert_to_tensor(score_list, dtype=tf.float32)\n",
        "  return sent_tensor\n",
        "\n",
        "def Model_build(X,Y,X_predict,Y_predict,batches_size=100):\n",
        "  ops.reset_default_graph()\n",
        "  Model = Sequential()\n",
        "  #Model.add(Dense(20, activation='relu'))\n",
        "  #Model.add(Dropout(0.3))\n",
        "  Model.add(Dense(5, activation='relu'))\n",
        "  #Model.add(Dropout(0.3))\n",
        "  Model.add(Dense(1, activation='sigmoid'))\n",
        "  #Model.add(Dense(1))\n",
        "  #Model.add(Activation('softmax'))\n",
        "\n",
        "  opt = optimizers.Adam(lr=0.005, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "\n",
        "  Model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])\n",
        "  #Model.summary()\n",
        "\n",
        "  steps_test = int(len(X)/batches_size)\n",
        "  steps_val = int(len(X_predict)/batches_size)\n",
        "\n",
        "  Model.fit(X, Y, epochs=3, batch_size=batches_size)\n",
        "\n",
        "  #print('The train accuracies are as given below :')\n",
        "  #print(History.history['accuracy'])\n",
        "  Model.save('/content/imdb/model')\n",
        "  sentiment_score = Model.predict(X_predict, batch_size=batches_size, steps=steps_val, workers=3)\n",
        "  return sentiment_score"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTON8_HeWKAV",
        "colab_type": "text"
      },
      "source": [
        "1) Name can be 'word2vec' or 'doc2vec' or 'lsi' or 'tfidf' or 'glove' or 'cooccur'.\n",
        "\n",
        "2) location is the path to input .csv file\n",
        "\n",
        "3) output_location is the path to save the processed data\n",
        "\n",
        "4) dict_location is the path to dictionary of LSI or TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajnRU9EfWhmP",
        "colab_type": "text"
      },
      "source": [
        "Change name, output_location for all models and dict_location for LSI and TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXdIQWmIGFOG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#data_module(reviews=False,name='cooccur',max_seq_length=100,location='/content/drive/My Drive/Movielensdata/ml25m/ratings.csv',output_location='/content/drive/My Drive/Movielensdata/ml25m/cooccur/data', dict_location='/content/drive/My Drive/Movielensdata/ml25m/tfidf/dict')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaH5acqZYYI5",
        "colab_type": "text"
      },
      "source": [
        "Change name, Data_location and model_save_location for all models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iWoBWvRZwSD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#embedding_model(name='cooccur',Data_location='/content/drive/My Drive/Movielensdata/ml25m/cooccur/data',model_save_location='/content/drive/My Drive/Movielensdata/ml25m/cooccur/co_occur', matrix_location='/content/drive/My Drive/Movielensdata/ml25m/cooccur/matrix', vector_dims=10,Sg=1,size_window=3,topics=10,mini_count=1,num_workers=3,max_num_epochs = 10,alpha = 0.025,min_alpha=0.00025,dm=1,maxi_features=None)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WqzalHzc_lp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#scoring_input = [['899', '2161', '3949', '5878', '1175', '1237', '8154', '2843', '7365', '4422', '6016'],['1080', '3114', '3671', '2791', '1288', '1', '541', '2692', '7323', '8014', '6370', '4703', '5147']]\n",
        "#save_object(scoring_input,'/content/drive/My Drive/Movielensdata/ml25m/scoring_input')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vuSkxUwYf8J",
        "colab_type": "text"
      },
      "source": [
        "Change name, model_loc or all models and dict_loc for LSI and TF-IDF "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ry_oK4hcUcJb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#scored_list = scoring_module(name='cooccur', model_loc='/content/drive/My Drive/Movielensdata/ml25m/cooccur/co_occur', data_loc='/content/drive/My Drive/Movielensdata/ml25m/scoring_input', dict_loc='/content/drive/My Drive/Movielensdata/ml25m/tfidf/dict', matrix_location ='/content/drive/My Drive/Movielensdata/ml25m/cooccur/matrix' )"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uld4mvrZfVMm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = load_object('/content/imdb/data')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_4LF_ieeXvW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_module(reviews=True, name='word2vec',max_seq_length=100,location='/content/imdb/data',output_location='/content/imdb/train_sent', dict_location='/content/drive/My Drive/Movielensdata/ml25m/tfidf/dict')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EsUKDzuXFFL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "0da0a4b2-2eff-4ffe-8239-e3cd636e8eed"
      },
      "source": [
        "embedding_model(name='word2vec',Data_location='/content/imdb/train_sent',model_save_location='/content/imdb/trained_model', matrix_location='/content/drive/My Drive/Movielensdata/ml25m/cooccur/matrix', vector_dims=10,Sg=1,size_window=3,topics=10,mini_count=1,num_workers=3,max_num_epochs = 10,alpha = 0.025,min_alpha=0.00025,dm=1,maxi_features=None)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIR9yYbeb_ES",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "5e0e98a1-ffde-4661-8638-cc328ee6a8db"
      },
      "source": [
        "scored_test= scoring_module(name='word2vec', model_loc='/content/imdb/trained_model', data_loc='/content/imdb/test_sent', dict_loc='/content/drive/My Drive/Movielensdata/ml25m/tfidf/dict', matrix_location ='/content/drive/My Drive/Movielensdata/ml25m/cooccur/matrix' )"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BbXZZXvcU1H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "a1bdc3a2-91cf-4154-f90d-2d1ebca675ef"
      },
      "source": [
        "scored_val = scoring_module(name='word2vec', model_loc='/content/imdb/trained_model', data_loc='/content/imdb/val_sent', dict_loc='/content/drive/My Drive/Movielensdata/ml25m/tfidf/dict', matrix_location ='/content/drive/My Drive/Movielensdata/ml25m/cooccur/matrix' )"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWEvzm4ncfeP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_tensor = sent_embedding(name='word2vec',score_list=scored_test)\n",
        "val_tensor = sent_embedding(name='word2vec',score_list=scored_val)\n",
        "\n",
        "Y_test = load_object('/content/imdb/Y_test')\n",
        "Y_val = load_object('/content/imdb/Y_val')"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Krs-h734cvcF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "e75a0499-3c9e-4573-ec24-4ae9bd8f37f9"
      },
      "source": [
        "sentiment_scores = Model_build(X=test_tensor, Y=Y_test, X_predict=val_tensor, Y_predict=Y_val,batches_size=100)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 5.9390e-08 - accuracy: 0.4983\n",
            "Epoch 2/3\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 5.9390e-08 - accuracy: 0.4983\n",
            "Epoch 3/3\n",
            "150/150 [==============================] - 0s 1ms/step - loss: 5.9390e-08 - accuracy: 0.4983\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "INFO:tensorflow:Assets written to: /content/imdb/model/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6ERYJ81pEEO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "5e6fa0b4-056c-4413-c59b-18c8a99c874a"
      },
      "source": [
        "val_tensor[0:2]"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 10), dtype=float32, numpy=\n",
              "array([[ 1.2374394 ,  0.5226084 , -0.46346664,  0.19295035, -0.17162189,\n",
              "        -0.2756378 ,  0.92306423,  0.07790467, -0.12668224, -1.0604434 ],\n",
              "       [ 1.1727355 ,  0.40256056, -0.3758635 ,  0.18853407, -0.1338653 ,\n",
              "        -0.32463962,  1.0611571 ,  0.28185797, -0.14266172, -0.9698675 ]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJiPaOzUeI9P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "6ebe6a07-6d23-4d1e-d825-0fa5c55f3f3f"
      },
      "source": [
        "sentiment_scores[0:10]"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.5456917 ],\n",
              "       [0.54720646],\n",
              "       [0.5396054 ],\n",
              "       [0.55762064],\n",
              "       [0.5279729 ],\n",
              "       [0.54478204],\n",
              "       [0.57946515],\n",
              "       [0.5448287 ],\n",
              "       [0.5637503 ],\n",
              "       [0.54343474]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkOnPLL1eKZR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "029b9e97-3498-4576-bef8-571f0a39a29b"
      },
      "source": [
        "Y_val[0:10]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, 1, 0, 0, 0, 1, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oUntAJ0oJ4N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvuJPnoYoKKI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLwFy4sMoKY8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}